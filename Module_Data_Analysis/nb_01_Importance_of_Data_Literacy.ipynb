{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "**Intuition** fails in many situations where **uncertainty** is present.  Coupled with **innumeracy**, this leads to poor decisions because risks and costs are not accurately evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:08:34.929676Z",
     "start_time": "2023-01-05T19:08:34.417953Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from colorama import Back, Fore, Style\n",
    "from pathlib import Path\n",
    "from sys import path\n",
    "\n",
    "path.append( str(Path.cwd().parent) )\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:08:36.204219Z",
     "start_time": "2023-01-05T19:08:36.155644Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:08:38.034239Z",
     "start_time": "2023-01-05T19:08:37.671048Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import scipy.stats as stats\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from Amaral_libraries.my_stats import half_frame \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Statistics\n",
    "\n",
    "If this is your first class on the topic, you may be wondering those names mean and what is the difference between these topics. Maybe, even if this is not your fist class on the topic, you may still be wondering.\n",
    "\n",
    "**Probability** deals with calculating the likelihood of something occurring, or if the event is likely, how frequently it will occur. Typically, this assumes that if one were able to wait an infinite amount of time, one would learn the true likelihood of something occurring. As such, it deals, **in some interpretations**, with idealized processes: \n",
    "\n",
    "> perfect fair dice that could presumably be rolled an infinite number of times \n",
    "\n",
    "or, equivalently, \n",
    "\n",
    "> an infinite number of perfectly identical fair dice that could all be rolled simultaneously.\n",
    "\n",
    "**Statistics** deals with the process of learning from experience, that is, from data.  \n",
    "\n",
    "> Experience is not infinite. \n",
    ">\n",
    "> Processes are not necessarily stationary.  \n",
    "\n",
    "So, can we can only really hope to learn something about the **current** world with some **desired degree of confidence** that can then be used to make optimal **decisions**.\n",
    "\n",
    "**Probability theory supports statistics by:** \n",
    "\n",
    "> providing models for describing stochastic processes\n",
    ">\n",
    "> providing a method for determining degrees of confidence on statements about stochastic processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeracy\n",
    "\n",
    "Statistics help us think critically and rationally about data and numbers. **It is impossible to be an informed citizen of a democracy while being innumerate**.\n",
    "\n",
    "Innumerate people think they know whether something is small or large in the absence of context.\n",
    "\n",
    "Is 1/5in large?\n",
    "\n",
    "One cannot answer without having a context.  Imagine that sea levels rise by 1/5in a year (about the current estimate). In 50 years they would have risen 10in, nearly a foot.  How many people would be affected bu such a rise in sea levels? A recent [study](https://www.nature.com/articles/s41467-019-12808-z) estimates that 230 million people currently live within 1 meter of current high tide levels.\n",
    "\n",
    "> Innumerate people make political strategy recommendations based on changes in polling results of candidates of a few tenths of a percentage point.\n",
    ">\n",
    "> Innumerate people are easy targets for scams because they do not realize when the game is not fair.\n",
    ">\n",
    "> Innumerate people take unnecessary risks and make incorrect cost-benefit assessments.\n",
    "> \n",
    "> Innumerate people do not realize that inappropriate data will focus attention on the wrong thing and make problems worse.\n",
    "> \n",
    "> Innumerate people do not realize that skewed distributions make some statistics such as the average atypical.\n",
    "\n",
    "## What do business people know?\n",
    "\n",
    "Nothing. Nothing at all.\n",
    "\n",
    "<img src = \"Images/2x2_hbr_stupid_matrix.png\" width = 300>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The challenge of uncertainty\n",
    "\n",
    "Economists have, for a long time, studied *homo economicus*. This strange species is omniscient and able easily and quickly to perform any calculation.  Unlike *economicus*, *sapiens* typically has access to incomplete or even erronious information and rather more limited computing abilities.\n",
    "\n",
    "In fact, in more practical situations, we cannot obtain all necessary information to know with certainty what is the current state of the world.  Consider elections.  You can imagine how a candidate would love to have access to daily updates on the number of votes that they and their rivals would receive if the election was to occur on that day. With that information, they would be able to determine whether their electoral strategy was working or not, and test which changes would improve their ability to receive the most votes.\n",
    "\n",
    "Getting such an update would be prohibitively expensive.  Daily contacting and receiving vote intentions from every potential voter in a given area would require huge numbers of people constantly contacting potential votes.  Even for a small state such as New Hampshire, that would mean contacting on the order to 400 thousand individuals daily.  \n",
    "\n",
    "Moreover, even if you could contact all of those people, there would be no assurance that they would answer your question or reveal their true voting intentions. \n",
    "\n",
    "That is why no one runs election polls in that manner. Instead polling organizations survey much smaller groups rather infrequently.  Consider some poll results for the 2020 Democratic Presidential Primary in New Hampshire.\n",
    "\n",
    "<img src = \"Images/poll_results.png\" width = '600'>\n",
    "\n",
    "Notice that most polls survey about 700 people and that even that small number of people are reached over a period of several days.   Notice also that even though the polls where all taken in the same period, the voting intentions for Sanders varied between 24% and 31% whereas for Buttigieg, they varied between 21% and 25%.\n",
    "\n",
    "Some times, such poll results are presented with even more digits (25.3%) suggesting a degree of certainty that seems unwarranted. Most time, at the end of the press release there is a statement about the results having an uncertainty of $\\pm 3%$.\n",
    "\n",
    "**What does this means?**\n",
    "\n",
    "We will use the computer the simulate the polling process and see whether we can determine where the uncertainty comes from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:08:49.243950Z",
     "start_time": "2023-01-05T19:08:49.198894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "n_voters = 400000\n",
    "real_intentions = { 'Sanders': 0.26, 'Buttigieg': 0.24, 'Klobuchar': 0.2,\n",
    "                    'Warren': 0.09, 'Biden': 0.08 }\n",
    "\n",
    "poll_size = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:08:50.738777Z",
     "start_time": "2023-01-05T19:08:50.695052Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate a poll\n",
    "\n",
    "def create_voters(n_voters, real_intentions):\n",
    "    \"\"\"\n",
    "    Given a population size and voter intentions, this functions\n",
    "    creates a list of voters characterized by their expected \n",
    "    candidate selections\n",
    "    \n",
    "    inputs:\n",
    "        n_voters -- int\n",
    "        real_intentions -- dict with candidate's name as key and \n",
    "                           probabilty of getting a vote as value    \n",
    "\n",
    "    outputs:\n",
    "        voters -- list of the candidates selected by each voter\n",
    "    \"\"\"\n",
    "    voters = []\n",
    "    for candidate in real_intentions:\n",
    "        voters.extend( [candidate] * \n",
    "                       int(n_voters * real_intentions[candidate]) )\n",
    "    \n",
    "    voters.extend( ['Other'] * (n_voters - len(voters)) )\n",
    "\n",
    "    if len(voters) != n_voters:\n",
    "        return None\n",
    "    \n",
    "    return voters\n",
    "    \n",
    "def simulate_poll(voters, poll_size):\n",
    "    \"\"\"\n",
    "    Extracts a sample of size poll_size from the list voters. \n",
    "    \n",
    "    inputs:\n",
    "        voters -- list of the candidates selected by each voter\n",
    "        poll_size -- int\n",
    "        \n",
    "    returns:\n",
    "        poll_results -- list of candidates selected by sampled voters \n",
    "        counter -- collections.Counter object\n",
    "    \"\"\"\n",
    "    preferences = random.sample(voters, poll_size)\n",
    "        \n",
    "    return preferences, Counter(preferences)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:08:51.801551Z",
     "start_time": "2023-01-05T19:08:51.761366Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_preferences = {'me': 1}\n",
    "# test_preferences = {'me': 0.9}\n",
    "test_preferences = {'me': 0.5, 'you': 0.5}\n",
    "\n",
    "voters = create_voters(1000, test_preferences)\n",
    "results, counter = simulate_poll(voters, 50)\n",
    "\n",
    "print(f\"The voting results were:\\n\\t{results}\\n\")\n",
    "print(counter)\n",
    "print(f\"\\n--- The counter object is of type {type(counter)}.\")\n",
    "\n",
    "print(f\"\\nI got {counter['me']} votes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:08:53.085351Z",
     "start_time": "2023-01-05T19:08:53.012022Z"
    }
   },
   "outputs": [],
   "source": [
    "voters = create_voters(n_voters, real_intentions)\n",
    "\n",
    "# Run many polls\n",
    "#\n",
    "n_polls = 100\n",
    "poll_size = 500\n",
    "sanders_results = []\n",
    "\n",
    "for i in range(n_polls):\n",
    "    results, counter = simulate_poll(voters, poll_size)\n",
    "    \n",
    "    sanders_results.append(counter['Sanders'] / poll_size * 100)\n",
    "\n",
    "print(f\"The first 5 results for Sanders were:\\t{sanders_results[:5]}\\n\")\n",
    "print(stats.describe(sanders_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:08:55.177010Z",
     "start_time": "2023-01-05T19:08:54.993041Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize = (6, 4) )\n",
    "ax = fig.add_subplot( 111 )\n",
    "half_frame(ax, \"Sanders' poll result\", 'Frequency', font_size = my_fontsize)\n",
    "\n",
    "ax.hist( sanders_results, bins = np.arange(0, 40, 1), color = 'gray',\n",
    "         align = 'mid', rwidth = 0.9 )\n",
    "\n",
    "ax.set_ylim(0, 30)\n",
    "ax.set_xlim(15, 35)\n",
    "ax.set_xticks(list(np.arange(15, 35, 5)))\n",
    "\n",
    "ax.vlines(np.mean(sanders_results), 0, 30, color = 'k', lw = 2)\n",
    "\n",
    "ax.text(16, 22, (f\"Poll size: {poll_size}\\n\"\n",
    "                 f\"Mean: {np.mean(sanders_results):>.1f}\\n\"\n",
    "                 f\"Std. dev.: {np.std(sanders_results):>.1f}\"), \n",
    "        fontsize = my_fontsize)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers I used for the voting intentions are pretty much the results from the actual election\n",
    "\n",
    "<img src = \"Images/election_results.png\" width = '500'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting pandemics\n",
    "\n",
    "When the pandemic got going in early Spring 2020, there was a lot of uncertainty about what its impact would be in the US.  A research institute at the University of Washington $-$ the Institute for Health Metrics and Evaluation (IHME) $-$ was particularly well placed for preparing forecast because of their expertise on data collection and modeling. Not surprisingly, especially because of the ineptitude displayed by the  Centers for Diseases Control (CDC), their forecasts received a lot of attention by the US media. \n",
    "\n",
    "Below, I should two graphs representative of those produced by the IHME team.  You will notice that there is a well defined uncertainty.  That seems good.\n",
    "\n",
    "There is a GIGANTIC problem, however. **This uncertainty vanishes as one forecasts further away from the current date.** \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src = 'Images/covid_deaths_ihme2.png' width = 440> </td>\n",
    "        <td> <img src = 'Images/covid_deaths_ihme.png' width = 400> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "How could this be? How could we be so confident about the future dynamics of a process about which, at the time, we knew so little about?\n",
    "\n",
    "**Indeed, that is not at all how the pandemic progressed.**\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src = 'Images/covid_deaths.png' width = 450>  \n",
    "</center> \n",
    "\n",
    "\n",
    "**What kind of unknowns were the modelers at IHME forgetting to take into consideration?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting on a spaceship to the Moon\n",
    "\n",
    "Some people like to own new cars because everything is new and thus less likely to fail.  With an old, used car, you do not know whether the transmission is about to go kaput, or whether the spark plugs are dying, or if the battery is in its last legs.\n",
    "\n",
    "The risks are naturally heightened if, instead of dealing with a drive to the grocery store, we are dealing with a long road trip.\n",
    "\n",
    "And the risk of component failure are even more consequential if instead of driving, we are flying across the Atlantic, or on a spaceship headed for the Moon.\n",
    "\n",
    "If the ship's 'spark plugs' fail, we would be in real trouble.\n",
    "\n",
    "For this reason, it is really important to characterize things such as failure rates of components so that we can provide enough redundancy in our system so that any single component failure is not critical.\n",
    "\n",
    "It turns out that in many many situations, the failure rate of 'simple' components behaves as a **geometric distribution**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric distribution \n",
    "\n",
    "The geometric distribution is the result of considering a statistical process in which you count the number of events until a specific outcome occurs.  Imagine that you are rolling a die.  **How many rolls can you expect to have to make until you get a `1`?**\n",
    "\n",
    "Assuming we are dealing with a fair die, than the probability of rolling a `1` is 1/6.  Then, the probability of having to wait `k` rolls until getting a `1` is:\n",
    "\n",
    "> $\\left( \\frac{5}{6} \\right)^{k-1}~.~\\frac{1}{6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:02.261874Z",
     "start_time": "2023-01-05T19:09:02.147727Z"
    }
   },
   "outputs": [],
   "source": [
    "p = 1/6\n",
    "x = np.arange(stats.geom.ppf(0, p), stats.geom.ppf(0.999, p))\n",
    "rv1 = stats.geom(p)\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize = (7, 4) )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "half_frame(ax, \"k\", \"Probability mass\", font_size = my_fontsize)\n",
    "\n",
    "# Calculate and plot histogram\n",
    "ax.vlines(x, 0, rv1.pmf(x), color = \"g\", linewidth = 5, alpha = 0.5) \n",
    "\n",
    "ax.set_ylim(0, 0.2)\n",
    "ax.set_xlim(0, 30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as I explained, you are going to be traveling on a rocket ship to the Moon. The trip is going to take 3 days.\n",
    "\n",
    "You are concerned because a particular component has a 0.05 probability of failure during any hour of operation. How much redundancy would you require?\n",
    "\n",
    "First, let us see what is the probability that the component avoids failure in the course of the three days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:03.775820Z",
     "start_time": "2023-01-05T19:09:03.734093Z"
    }
   },
   "outputs": [],
   "source": [
    "prob_failure = 0.05\n",
    "n_attempts = 1000\n",
    "\n",
    "failed = 0\n",
    "for i in range(n_attempts):\n",
    "    for k in range(3 * 24):\n",
    "        if random.random() < 0.05:\n",
    "            failed += 1\n",
    "            break\n",
    "\n",
    "print(f\"In {failed} out of {n_attempts} attempts, the component failed \"\n",
    "      f\"too early.\\nThat is {100*failed/n_attempts:.1f}% of the time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not good.  I am not getting on that ship.\n",
    "\n",
    "What if we have `n_components` in parallel for redundancy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:07.553042Z",
     "start_time": "2023-01-05T19:09:07.443941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trade-off parameters: \n",
    "#      decreasing prob_failure is more expensive than adding\n",
    "#      more components. More components add weight and complexity.\n",
    "#\n",
    "prob_failure = 0.005\n",
    "n_components = 10\n",
    "\n",
    "n_worlds = 1000\n",
    "failed = 0\n",
    "for i in range(n_attempts):\n",
    "    failed_components = 0\n",
    "    for component in range(n_components):\n",
    "        for k in range(3 * 24):\n",
    "            if random.random() < prob_failure:\n",
    "                failed_components += 1\n",
    "                break\n",
    "        if failed_components == n_components:\n",
    "            failed += 1\n",
    "            break\n",
    "\n",
    "print(f\"In {failed} out of {n_attempts} attempts, the system failed \"\n",
    "      f\"too early.\\nThat is {100*failed/n_attempts:.1f}% of the time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great is it?  Because we have crappy components, we need huge amounts of them to avoid catastrophe.  One to to consider is that redundancy is adding to the weight of the spaceship, which will add to the cost.\n",
    "\n",
    "**Should we pay more for better components or pay more for operating a heavier spaceship?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When aggregate data hide the true pattern\n",
    "\n",
    "There is a famous result in statistics that goes by the name of [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox). Danielle Navarro provides a particularly good example in her [Learning Statistics with R](https://learningstatisticswithr.com/book/why-do-we-learn-statistics.html) book which I quote:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<p style = \"width: 80%\">\n",
    "\"In 1973, the University of California, Berkeley had some worries about the admissions of students into their postgraduate courses. Specifically, the thing that caused the problem was that the gender breakdown of their admissions, which looked like this...\"\n",
    "\n",
    "<table>\n",
    "<tr> \n",
    "    <td> </td> <td><b>Number of applicants</b></td> <td><b>Admitted</b></td> \n",
    "</tr>\n",
    "<tr> \n",
    "    <td> <b>Males</b> </td> <td>8,442</td> <td>46%</td> \n",
    "</tr>\n",
    "<tr> \n",
    "    <td> <b>Females</b> </td> <td>4,321</td> <td>35%</td> \n",
    "</tr>        \n",
    "</table>\t \t\n",
    "</center>\n",
    "\n",
    "These numbers seem pretty damning of the admission process at Berkeley.  However, the story is a bit more subtle. If one looks at the number broken down by department, the picture is radically changed.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<table>\n",
    "<tr> \n",
    "    <td><b>Department</b></td> <td><b>Male Applicants</b></td> <td><b>Admitted</b></td> \n",
    "    <td><b>Female Applicants</b></td> <td><b>Admitted</b></td>\n",
    "</tr>\n",
    "<tr> \n",
    "    <td> <b>A</b> </td> <td>825</td> <td>62%</td> <td>108</td> <td>82%</td>\n",
    "</tr>\n",
    "<tr> \n",
    "    <td> <b>B</b> </td> <td>560</td> <td>63%</td> <td>25</td> <td>68%</td>\n",
    "</tr>    \n",
    "<tr> \n",
    "    <td> <b>C</b> </td> <td>325</td> <td>37%</td> <td>593</td> <td>34%</td>\n",
    "</tr>    \n",
    "<tr> \n",
    "    <td> <b>D</b> </td> <td>417</td> <td>33%</td> <td>375</td> <td>35%</td>\n",
    "</tr>    \n",
    "<tr> \n",
    "    <td> <b>E</b> </td> <td>191</td> <td>28%</td> <td>393</td> <td>24%</td>\n",
    "</tr>    \n",
    "<tr> \n",
    "    <td> <b>F</b> </td> <td>272</td> <td>6%</td> <td>341</td> <td>7%</td>\n",
    "</tr>  \n",
    "</table>\t \t    \n",
    " \n",
    "<br>\n",
    "<p style = \"width: 80%\">\n",
    "Remarkably, most departments had a higher rate of admissions for females than for males! Yet the overall rate of admission across the university for females was lower than for males. How can this be? How can both of these statements be true at the same time?\n",
    "\n",
    "<br>\n",
    "<p style = \"width: 80%\">\n",
    "Here's what's going on. Firstly, notice that the departments are not equal to one another in terms of their admission percentages: some departments [...] tended to admit a high percentage of the qualified applicants, whereas others [...] tended to reject most of the candidates, even if they were high quality. So, among the six departments shown above, notice that department A is the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females tended to apply to different departments. If we rank the departments in terms of the total number of male applicants, we get A > B > D > C > F > E. On the whole, males tended to apply to the departments that had high admission rates.\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the math adds up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:11.140361Z",
     "start_time": "2023-01-05T19:09:11.092151Z"
    }
   },
   "outputs": [],
   "source": [
    "male_applicants = np.array([825, 560, 325, 417, 191, 271])\n",
    "male_admission_fractions = np.array([0.62, 0.63, 0.37, 0.33, 0.29, 0.06])\n",
    "total_male_applicants = sum(male_applicants)\n",
    "total_male_admits = sum(male_admission_fractions * male_applicants)\n",
    "\n",
    "print(f\"For these six departments, there were {total_male_applicants} \"\n",
    "      f\"male applicants and {int(total_male_admits)} admits.\\n\")\n",
    "\n",
    "overall_male_admission_rate = total_male_admits / total_male_applicants\n",
    "\n",
    "print(f\"This yields an overall male admission rate of \"\n",
    "      f\"{100*overall_male_admission_rate:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:11.785472Z",
     "start_time": "2023-01-05T19:09:11.741338Z"
    }
   },
   "outputs": [],
   "source": [
    "female_applicants = np.array([108, 25, 593, 375, 393, 341])\n",
    "female_admission_fractions = np.array([0.82, 0.68, 0.34, 0.35, 0.24, 0.07])\n",
    "total_female_applicants = sum(female_applicants)\n",
    "total_female_admits = sum(female_admission_fractions * female_applicants)\n",
    "\n",
    "print(f\"For these six departments, there were {total_female_applicants} \"\n",
    "      f\"female applicants and {int(total_female_admits)} admits.\\n\")\n",
    "\n",
    "overall_female_admission_rate = total_female_admits / total_female_applicants\n",
    "\n",
    "print(f\"This yields an overall female admission rate of \"\n",
    "      f\"{100*overall_female_admission_rate:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elon Must enters a bar...\n",
    "\n",
    "and suddenly the **average** patron is a fucking sub-human piece of sociopathic shit.\n",
    "\n",
    "No, wait, I meant *a billionaire*.\n",
    "\n",
    "When we talk about the **average Joe**, we imagine someone that is typical and representative. Consider all your classmates. What is your average height? Probably something like 5'8\"...  Most of you are likely somewhere between 5'4\" and 6'0\". About half will be taller than the average and half will be shorter.\n",
    "\n",
    "Our intuition about statistics is driven by the expected outcomes for the so-called normal or bell-curve distribution. Measurements aggregate around the average, so the average is quite representative of the population.\n",
    "\n",
    "The probability density function of the bell-curve distribution is:\n",
    "\n",
    "> $f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{- (x - \\mu)^2 / 2\\sigma^2}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:13.885245Z",
     "start_time": "2023-01-05T19:09:13.763337Z"
    }
   },
   "outputs": [],
   "source": [
    "mu = 5.67\n",
    "sigma = 0.5 \n",
    "x = np.linspace(stats.norm.ppf(0.0001, mu, sigma), stats.norm.ppf(0.9999, mu, sigma))\n",
    "f_x = stats.norm(mu, sigma)\n",
    "\n",
    "fig = plt.figure( figsize = (7, 4) )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "half_frame(ax, \"Height\", \"Probability density\", font_size = my_fontsize)\n",
    "\n",
    "ax.plot(x, f_x.pdf(x), color = \"r\", linewidth = 2, alpha = 0.9)\n",
    "ax.vlines(5.67, 0, 0.8, color = 'k', lw = 4, zorder = -10)\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In contrast, the distribution of wealth $-$ and of sociopathy $-$ is very right skewed**. A good description is a power law distribution. Its probability density function is:\n",
    "\n",
    "> $f(x; \\alpha) = \\frac{1}{\\alpha - 1} ~ x^{-\\alpha} ~~~~~$  \n",
    "\n",
    "where $x \\ge 1$  and $\\alpha > 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:15.463033Z",
     "start_time": "2023-01-05T19:09:15.040011Z"
    }
   },
   "outputs": [],
   "source": [
    "# This creates data with alpha = 2.\n",
    "#\n",
    "data = 1. / stats.uniform.rvs(0, 1, size = 100000)\n",
    "\n",
    "fig = plt.figure( figsize = (6, 4.5) )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "half_frame(ax, \"x\", \"Probability density\", font_size = my_fontsize)\n",
    "\n",
    "ax.hist( data, bins = np.geomspace(1, 1E6, num = 20), color = 'gray', \n",
    "         align = 'mid', rwidth = 0.8, density = True )\n",
    "\n",
    "ax.loglog()\n",
    "\n",
    "ax.set_ylim(1E-10, 1)\n",
    "ax.set_xlim(1, 1E6)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floodings\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src = 'Images/flooding_germany.png' width = 350> </td>\n",
    "        <td> <img src = 'Images/flooding_pakistan.png' width = 440> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<center>\n",
    "<img src = 'Images/flooding_pakistan2.png' width = 450>  \n",
    "</center> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earthquakes\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src = 'Images/earthquake_japan.png' width = 350> </td>\n",
    "        <td> <img src = 'Images/earthquake.png' width = 440> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fires\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src = 'Images/fires_mediterranean-2021.png' width = 400> </td>\n",
    "        <td> <img src = 'Images/fires_europe-2016.png' width = 400> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<center>\n",
    "    <img src = 'Images/firenado_guardian.png' width = 500>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volcanic eruptions \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src = 'Images/volcanic_eruptions.png' width = 400> </td>\n",
    "        <td> <img src = 'Images/volcanic_eruptions_graph.png' width = 400> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<center>\n",
    "    <img src = 'Images/volcano.png' width = 500>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our intuition fails us\n",
    "\n",
    "Looking at the functional form of the distribution, or at those examples, actually does not fully show us **how unreliable** the statistical descriptors obtained from small samples are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:20.208177Z",
     "start_time": "2023-01-05T19:09:19.839741Z"
    }
   },
   "outputs": [],
   "source": [
    "means = []\n",
    "st_devs = []\n",
    "for i in range(100):\n",
    "    data = 1. / stats.uniform.rvs(0, 1, size = 100)\n",
    "    means.append(np.mean(data))\n",
    "    st_devs.append(np.std(data))\n",
    "\n",
    "fig = plt.figure( figsize = (10, 4.5) )\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "half_frame(ax1, \"Sample\", \"Mean\", font_size = my_fontsize)\n",
    "ax1.semilogy(means)\n",
    "ax1.set_xlim(0, 100)\n",
    "\n",
    "half_frame(ax2, \"Sample\", \"Standard deviation\", font_size = my_fontsize)\n",
    "ax2.semilogy(st_devs)\n",
    "ax2.set_xlim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"The smallest mean is {min(means):.1f}, the largest is {max(means):.1f}\\n\")\n",
    "print(f\"The smallest st. dev. is {min(st_devs):.1f}, the largest is {max(st_devs):.1f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, **the mean can change by a factor of 30 and the standard deviation by a factor of 300!**\n",
    "\n",
    "\n",
    "This is the similar analysis with a normal process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:09:21.860768Z",
     "start_time": "2023-01-05T19:09:21.659782Z"
    }
   },
   "outputs": [],
   "source": [
    "means = []\n",
    "st_devs = []\n",
    "for i in range(100):\n",
    "    data = stats.norm.rvs(500, 50, size = 100)\n",
    "    means.append(np.mean(data))\n",
    "    st_devs.append(np.std(data))\n",
    "\n",
    "fig = plt.figure( figsize = (10, 4.5) )\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "half_frame(ax1, \"Sample\", \"Mean\", font_size = my_fontsize)\n",
    "ax1.plot(means)\n",
    "ax1.set_ylim(0, 1000)\n",
    "ax1.set_xlim(0, 100)\n",
    "\n",
    "half_frame(ax2, \"Sample\", \"Standard deviation\", font_size = my_fontsize)\n",
    "ax2.plot(st_devs)\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_xlim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"The smallest mean is {min(means):.1f}, the largest is {max(means):.1f}\\n\")\n",
    "print(f\"The smallest st. dev. is {min(st_devs):.1f}, the largest is {max(st_devs):.1f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the y-axis has a linear scale now. Variability is just a few percent.  That is why normally distributed stochastic processes give meaning to our intuition of what typical means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
