{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "\n",
    "This notebook considers multivariable stochastic processes.  It reviews the concept of joint distributions, marginal distributions, and describes how to estimate correlations between pairs of variables.\n",
    "\n",
    "In particular, it describes Pearson's $R$, Spearman's $\\rho$, and Kendall's $\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T20:48:57.550673Z",
     "start_time": "2023-08-04T20:48:57.022079Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from colorama import Back, Fore, Style\n",
    "from copy import copy, deepcopy\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T20:48:59.931473Z",
     "start_time": "2023-08-04T20:48:57.612371Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "from operator import itemgetter\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from module_libraries.my_stats import half_frame, playing_with_dice\n",
    "\n",
    "my_fontsize = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate processes\n",
    "\n",
    "Many processes generate multiple random variables. For example, the weather \"creates\" sets of variables such as temperature, pressure, wind speed, wind direction, humidity, ozone levels, particulate levels, and so on. In such a case, we talk of joint probability density (or mass) functions.\n",
    "\n",
    "## Joint distributions\n",
    "\n",
    "Let's focus on the case of continuous variables, as the situation is similar for discrete or mixed cases.  The **joint probability density function** satisfies the following properties:\n",
    "\n",
    "> 1. $f_{XY}(x, y) \\ge 0$ for all $x, y$\n",
    ">\n",
    ">\n",
    "> 2. $\\int_{S_X} ~ \\int_{S_Y} ~ f_{XY}(x, y) ~ dx~dy = 1$\n",
    ">\n",
    ">\n",
    "> 3. For any region $S$ contained in $S_X \\times S_Y$, \n",
    ">\n",
    "> $~~~~~~~~~~~~~~~~~~~~~~ P \\left((x,y) \\in S  \\right) = \\int \\int_S ~ f_{XY}(x, y) ~ dx~dy$\n",
    "\n",
    "\n",
    "where $X$ is a random variable, $x$ a possible value for $X$, and I use the notation $f_X(x)$ to mean $f(X = x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Consider an instrument with two identical independent parts set in parallel so as to provide redundancy.  The failure of each part is described by a exponential process with average time to failure of $\\tau = 1$ year.  \n",
    "\n",
    "1. What is the joint probability density function?\n",
    "\n",
    "2. What is the probability that the instrument will fail before 1 year?\n",
    "\n",
    "3. What if the parts where placed in serie?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Consider two dice, $X$ and $Y$.  \n",
    "\n",
    "1. What is the joint mass probability function?\n",
    "\n",
    "2. What is the probability that both dice return 2 or less?\n",
    "\n",
    "3. What is the probability that one of the die returns 2 or less?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T18:10:05.856588Z",
     "start_time": "2023-02-06T18:10:05.555417Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "L = 1000\n",
    "x = np.arange(1, n+1)\n",
    "\n",
    "die1_throws = stats.randint.rvs(0, n, size = L)\n",
    "die2_throws = stats.randint.rvs(0, n, size = L)\n",
    "\n",
    "# Calculate histogram\n",
    "#\n",
    "h = [0]*n\n",
    "hist = np.array([h]*n)\n",
    "\n",
    "for i, j in zip(die1_throws, die2_throws):\n",
    "    hist[i, j] += 1   \n",
    "hist = hist / L\n",
    "\n",
    "# Plot data\n",
    "#\n",
    "fig = plt.figure( figsize = (6, 4.5) )\n",
    "fig.text(0, 1, f\"Expectation is {1/36:.2f}\", fontsize = my_fontsize*1.5)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "my_font_size = 15\n",
    "half_frame(ax, '$die_1$', '$die_2$', font_size = my_font_size)\n",
    "\n",
    "temp = ax.imshow(hist, cmap = plt.cm.cividis, vmin = 0, vmax = 0.05)\n",
    "\n",
    "ax.plot(die1_throws, die2_throws, 'o', 'r', alpha = 0.5)\n",
    "\n",
    "cbar = ax.figure.colorbar( temp, ax = ax, fraction = .08, shrink = 0.9,\n",
    "                           ticks = [0, 0.01, 0.02, 0.030, 0.04, 0.05], )\n",
    "cbar.ax.set_ylabel( 'Probability', rotation = -90, va = \"bottom\", \n",
    "                    fontsize = my_font_size )\n",
    "\n",
    "ax.set_xticks(x - 1)\n",
    "ax.set_xticklabels(x)\n",
    "\n",
    "ax.set_yticks(x - 1)\n",
    "ax.set_yticklabels(x)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginal distributions\n",
    "\n",
    "Especially when considering probability density functions of many random variables, one may want to visualize what is going on with the probability for **each** of those variables. In that case, one wants to compute the marginal probability density function.  \n",
    "\n",
    "Taking the formulation above, the marginal probability density for $X$ is\n",
    "\n",
    "> $~~~~~~~~~~f_X (x) = \\int_{S_Y} ~ f_{XY}(x, y) ~ dy$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T18:10:15.244913Z",
     "start_time": "2023-02-06T18:10:15.017523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate marginal histograms\n",
    "#\n",
    "hist1 = np.array([0]*n)\n",
    "hist2 = np.array([0]*n)\n",
    "for i, j in zip(die1_throws, die2_throws):\n",
    "    hist1[i] += 1\n",
    "    hist2[j] += 1\n",
    "    \n",
    "hist1 = hist1 / L\n",
    "hist2 = hist2 / L\n",
    "\n",
    "# Plot data\n",
    "#\n",
    "ax = []\n",
    "my_font_size = 15\n",
    "fig = plt.figure( figsize = (6, 8) )\n",
    "\n",
    "ax.append( fig.add_subplot(2,1,1) )\n",
    "half_frame(ax[0], '$die_1$', 'Probability', font_size = my_font_size)\n",
    "ax[0].bar(x, hist1, color = 'b')\n",
    "ax[0].hlines(1/n, 0.5, n+0.5, color = 'k', linewidth = 4)\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(x)\n",
    "\n",
    "ax.append( fig.add_subplot(2,1,2) )\n",
    "half_frame(ax[1], '$die_2$', 'Probability', font_size = my_font_size)\n",
    "ax[1].bar(x, hist2, color = 'r')\n",
    "ax[1].hlines(1/n, 0.5, n+0.5, color = 'k', linewidth = 4)\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(x)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional distributions\n",
    "\n",
    "In the example of the instrument failure, the failure of each of the parts is independent of what is happening with the other part.  This means that the joint probability distribution is simply the product of the individual probability distributions. \n",
    "\n",
    "However, in many cases of interest, the two outcomes are **not** independent.  In that case, it becomes important to know the conditional probability distribution.  Using Bayes theorem, we can write\n",
    "\n",
    "> $~~~~~~~~~~ f_{Y|x}(y) = f (Y = y ~ | ~ X = x) = \\frac{ f_{XY}(x, y)}{f_X(x)}~~~~~~$ for $~~~~~~~f_X(x) > 0$\n",
    "\n",
    "\n",
    "Estimating the correlation between random variables is extremely important, as is its figuring out the mechanisms giving rise to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T18:10:23.311297Z",
     "start_time": "2023-02-06T18:10:23.039813Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_function0(die1_throws, die2_throws):\n",
    "    \n",
    "    points = die1_throws + die2_throws\n",
    "    y_max = 6 + 6\n",
    "    y_min = 2\n",
    "    y = np.arange(0, y_max + 1)\n",
    "    \n",
    "    return points, y, y_max, y_min\n",
    "    \n",
    "    \n",
    "L = 200\n",
    "points0 = playing_with_dice( L, 10, die1_throws, die2_throws, \n",
    "                             my_function0, 10, my_font_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation measures\n",
    "\n",
    "## Covariance and Pearson's $r$ (see [Wikipedia article](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient))\n",
    "\n",
    "The covariance between two random variables $X$ and $Y$ is defined as\n",
    "\n",
    "> $~~~~~~~~~~cov(X, Y) = \\sigma_{XY} = E[(X-\\mu_x)(Y-\\mu_Y)] = E[XY] - \\mu_X \\mu_Y$\n",
    "\n",
    "where\n",
    "\n",
    "> $~~~~~~~~~~E[g(X,Y)] = \\int_{S_X} \\int_{S_Y} ~ g(x,y)~f_{XY}(x, y) ~ dx ~ dy$ \n",
    "\n",
    "The method `numpy.cov` returns a `numpy.array` object with shape (2, 2) with an estimate of the covariance matrix. \n",
    "\n",
    "Pearson correlation coefficient, also called Pearson's $r$ is just the covariance normalized by the standard deviations of the two random variables\n",
    "\n",
    "> corr$(X, Y) = r_{XY} = \\frac{cov(X, Y)}{\\sigma_X~\\sigma_Y}$\n",
    "\n",
    "The Pearson correlation coefficient is symmetric: $r_{XY} = r_{YX}$. A key mathematical property of the Pearson correlation coefficient is that it is invariant under separate changes in location and scale in the two variables. That is, we may transform $X \\to a_0 + a_1X$ and transform $Y \\to b_0 + b_1Y$, where $a_0, a_1, b_0, b_1$ are constants with $a_1 b_1 > 0$, without changing the correlation coefficient (if $a_1 b_1 < 0$ then the correlation coefficient changes sign).\n",
    "\n",
    "The method `scipy.stats.pearsonr` returns estimates of both the correlation coefficient and the p-value. **The calculation of the p-value relies on the assumption that each dataset is normally distributed**. The values of the Pearson correlation coefficients are in $[-1,~1]$. Correlations equal to 1 or $-1$ correspond to data points lying exactly on a line (in the case of the sample correlation), or to a bivariate distribution entirely supported on a line (in the case of the population correlation).  \n",
    "\n",
    "<img src = \"Images/correlation_examples.png\" width = 600>\n",
    "\n",
    "**Note that covariance is a measure of linear association**.  Non-linear monotonic associations will result in non-zero covariances, but do not measure correctly the degree of association between the two variables.\n",
    "\n",
    "## Spearman's $\\rho$ (see [Wikipedia article](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient))\n",
    "\n",
    "Spearman's rank correlation coefficient or Spearman's $\\rho$ is a nonparametric measure of **rank correlation** (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described using a **monotonic function**. Spearman's coefficient is appropriate for both continuous and **discrete ordinal variables**, such  as you would get from survey answers using a Likert scale.\n",
    "\n",
    ">  $\\rho_{XY} = \\frac{cov(R(X), R(Y))}{\\sigma_{R(X)}~\\sigma_{R(Y)}}$ ,\n",
    "\n",
    "where $R(X)$ is the rank of $X$. \n",
    "\n",
    "The method `scipy.stats.spearmanr` returns estimates of both the correlation coefficient and the p-value. These values are obtained by calling the attributes .`coefficient` and `.pvalue` of the result. **The calculation of the p-value can be obtained considering a permutation test of the ranks or by defining the Fisher transform of $r$\n",
    "\n",
    "> $F(r) = \\frac{1}{2}~\\ln \\frac{1+r}{1 - r}$ ,\n",
    "\n",
    "which is approximately normally distributed with zero mean and standard deviation\n",
    "\n",
    "> $\\sqrt \\frac{1.06}{n - 3} $.\n",
    "\n",
    "If there are no repeated data values, a perfect Spearman correlation of +1 or $-1$ occurs when each of the variables is a perfect monotone function of the other. Moreover, \n",
    "\n",
    "> $\\sigma_{R(X)}~\\sigma_{R(Y)} = Var[R(X)] = Var[R(Y)] = \\frac{n^2 - 1}{12}$ ,\n",
    "\n",
    "where $n$ is the sample size.\n",
    "\n",
    "**Non-linear, or even piecewise linear,  non-monotonic associations can frequently yield no covariance between variables. In this case, considering the covariance of the rank of the random variables is not going to solve the problem at all.**\n",
    "\n",
    "## Kendall's $\\tau$ (see [Wikipedia article](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient))\n",
    "\n",
    "The Kendall rank correlation coeficient  or Kendall's $\\tau$ is an alternative to Spearman's rank correlation coefficient. It is also a nonparametric measure of **rank correlation**.  It is defined as\n",
    "\n",
    "> $\\tau_{XY} = \\frac{2}{n(n - 1)}~\\sum_{i < j}{\\rm sgn}(x_i - x_j)~{\\rm sgn}(y_i - y_j)$.\n",
    "\n",
    "The Kendall rank coefficient is often used as a test statistic in a statistical hypothesis test to establish whether two variables may be regarded as statistically dependent. This test is non-parametric, as it does not rely on any assumptions on the distributions of $X$ or $Y$ or the distribution of $(X,Y)$. \n",
    "\n",
    "The method `scipy.stats.kendalltau` returns estimates of both the correlation coefficient and the p-value. These values are obtained by calling the attributes .`coefficient` and `.pvalue` of the result. **The calculation of the p-value can be obtained considering a permutation test of the ranks or, for large $n$, by a approximating the distribution of $\\tau$ as a normal with zero mean and standard deviation**\n",
    "\n",
    "> $\\frac{2~(2n + 5)}{9n~(n-1)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T21:36:20.013364Z",
     "start_time": "2023-02-06T21:36:19.929830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Throw two dice L times\n",
    "L = 200\n",
    "n = 6\n",
    "die1_throws = stats.randint.rvs(1, n+1, size = L)\n",
    "die2_throws = stats.randint.rvs(1, n+1, size = L)\n",
    "\n",
    "points, y, y_max, y_min = my_function0(die1_throws, die2_throws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The throws of the two dice are uncorrelated.**  \n",
    "\n",
    "We thus expect a very small value of the correlation, and a non-significant p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T21:37:01.777918Z",
     "start_time": "2023-02-06T21:37:01.714317Z"
    }
   },
   "outputs": [],
   "source": [
    "result = stats.pearsonr(die2_throws, die1_throws)\n",
    "print(f\"Indeed, we find Pearson's r is {result[0]:.3f} with an estimated \"\n",
    "      f\"significance level of {result[1]:.6f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The function of the points of the two dice is correlated with the points of each dice.**\n",
    "\n",
    "We thus expect a large correlation, and a significant p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T21:45:02.572392Z",
     "start_time": "2023-02-06T21:45:02.521510Z"
    }
   },
   "outputs": [],
   "source": [
    "result = np.cov(points, die1_throws)\n",
    "print(f\"The covariance between the two random variables is {result[0,1]:.3f}\\n\")\n",
    "\n",
    "result = stats.spearmanr(points, die1_throws)\n",
    "print(f\"Spearman's rho is {result.correlation:.3f} with an estimated \"\n",
    "      f\"significance level of {result.pvalue:.6f}\\n\")\n",
    "\n",
    "result = stats.kendalltau(points, die1_throws)\n",
    "print(f\"Kendall's tau is {result.correlation:.3f} with an estimated \"\n",
    "      f\"significance level of {result.pvalue:.6f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T19:39:31.063506Z",
     "start_time": "2023-02-07T19:39:30.707894Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_function1(die1_throws, die2_throws):\n",
    "    \n",
    "    points = die1_throws*die1_throws + die2_throws\n",
    "    y_max = 6*6 + 6\n",
    "    y_min = 2\n",
    "    y = np.arange(0, y_max + 1)\n",
    "    \n",
    "    return points, y, y_max, y_min\n",
    "    \n",
    "# Throw two dice L times\n",
    "n = 6\n",
    "x = np.arange(1, n+1)\n",
    "L = 200\n",
    "\n",
    "die1_throws = stats.randint.rvs(1, n+1, size = L)\n",
    "die2_throws = stats.randint.rvs(1, n+1, size = L)\n",
    "\n",
    "points1 = playing_with_dice( L, n, die1_throws, die2_throws, my_function1, \n",
    "                             20, my_fontsize )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T19:43:26.781696Z",
     "start_time": "2023-02-07T19:43:26.499295Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_function2(die1_throws, die2_throws):\n",
    "    \n",
    "    points = (3.5 - die1_throws)*(3.5 - die1_throws) + die2_throws\n",
    "    y_max = 13\n",
    "    y_min = 0\n",
    "    y = np.arange(0, y_max + 1)\n",
    "    \n",
    "    return points, y, y_max, y_min\n",
    "    \n",
    "    \n",
    "\n",
    "# Throw two dice L times\n",
    "n = 6\n",
    "x = np.arange(1, n+1)\n",
    "L = 200\n",
    "\n",
    "die1_throws = stats.randint.rvs(1, n+1, size = L)\n",
    "die2_throws = stats.randint.rvs(1, n+1, size = L)\n",
    "\n",
    "points2 = playing_with_dice( L, n, die1_throws, die2_throws, my_function2, \n",
    "                             12, my_fontsize )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T19:41:25.572716Z",
     "start_time": "2023-02-07T19:41:25.516144Z"
    }
   },
   "outputs": [],
   "source": [
    "2.5**2 + 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation is not causation\n",
    "\n",
    "Since at least the time of the Kabbalah, there has been a tradition of finding meaning and associations between unrelated things. From those that [\"demonstrated\" that the dimensions of the three Giza pyramids are in perfect scale to some planetary distances in the solar system](https://www.ancient-origins.net/forum/great-pyramid-decoded-solar-system-changes-history-002631), to astrology and so on.\n",
    "\n",
    "There is even a [site](https://www.tylervigen.com/spurious-correlations) dedicated to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T21:56:04.924393Z",
     "start_time": "2023-02-06T21:56:03.810267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate N random data sets of size L drawn from whatever distribution you like\n",
    "#\n",
    "L = 20\n",
    "N = 200\n",
    "random_samples = []\n",
    "\n",
    "for i in range(N):\n",
    "    random_samples.append( stats.expon.rvs(0, 1, size = L) )\n",
    "\n",
    "print(f\"I have generated {len(random_samples)} samples of length \"\n",
    "      f\"{len(random_samples[0])}.\")\n",
    "\n",
    "# Calculate the correlation between every pair of samples\n",
    "#\n",
    "spearman_correlation = []\n",
    "for i in range(N-1):\n",
    "    sample1 = random_samples[i]\n",
    "    for j in range(i+1,N):\n",
    "        sample2 = random_samples[j]\n",
    "        correlation, p_value = stats.pearsonr(sample1, sample2)\n",
    "        my_tuple = (abs(correlation), abs(correlation)/correlation, \n",
    "                                      p_value, i, j)\n",
    "        \n",
    "        spearman_correlation.append( my_tuple )\n",
    "        \n",
    "# What is the maximum and minimum correlation coefficient your observe\n",
    "\n",
    "my_max = max(spearman_correlation, key = itemgetter(0))\n",
    "print(f\"The maximum correlation, {my_max[0]:.3f}, occurs for samples \"\n",
    "      f\"{my_max[3]} and {my_max[4]}.\")\n",
    "\n",
    "my_min = min(spearman_correlation, key = itemgetter(0))\n",
    "print(f\"The minimum correlation, {my_min[0]:.3f}, occurs for samples \"\n",
    "      f\"{my_min[3]} and {my_min[4]}.\")\n",
    "\n",
    "ax = []\n",
    "fig = plt.figure( figsize = (12, 8) )\n",
    "gs = gridspec.GridSpec(4, 1)\n",
    "\n",
    "ax.append( fig.add_subplot(gs[0]) )\n",
    "half_frame(ax[0], '', '$X_i, X_j$', font_size = my_font_size)\n",
    "ax[0].plot( np.arange(0,L), random_samples[my_max[3]], 'o-', color = 'orange', \n",
    "           alpha = 0.8, label = f\"i = {my_max[3]}\" )\n",
    "ax[0].plot( np.arange(0,L), random_samples[my_max[4]], 'o-', color = 'b', \n",
    "           alpha = 0.8, label = f\"j = {my_max[4]}\" )\n",
    "ax[0].set_xlim(-0.5,L)\n",
    "ax[0].set_xticks(range(0, L, 2))\n",
    "ax[0].legend(loc = 'best', fontsize = my_fontsize)\n",
    "\n",
    "ax.append( fig.add_subplot(gs[1]) )\n",
    "half_frame(ax[1], '', '$|X_i - X_j|$', font_size = my_font_size)\n",
    "ax[1].bar( np.arange(0,L), abs(random_samples[my_max[3]] - random_samples[my_max[4]]), \n",
    "           color = 'r', alpha = 0.8, width = 0.3 )\n",
    "ax[1].set_ylim(0,4)\n",
    "ax[1].set_xlim(-0.5,L)\n",
    "ax[1].set_xticks(range(0, L, 2))\n",
    "ax[1].hlines([2, 4], -0.5, L+0.5, color = '0.4', lw = 2)\n",
    "\n",
    "ax.append( fig.add_subplot(gs[2]) )\n",
    "half_frame(ax[2], '', '$X_i, X_j$', font_size = my_font_size)\n",
    "ax[2].plot( np.arange(0,L), random_samples[my_min[3]], 'o-', color = 'orange', \n",
    "           alpha = 0.8, label = f\"i = {my_min[3]}\" )\n",
    "ax[2].plot( np.arange(0,L), random_samples[my_min[4]], 'o-', color = 'b', \n",
    "           alpha = 0.8, label = f\"j = {my_min[4]}\" )\n",
    "ax[2].set_xlim(-0.5,L)\n",
    "ax[2].set_xticks(range(0, L, 2))\n",
    "ax[2].legend(loc = 'best', fontsize = my_fontsize)\n",
    "\n",
    "ax.append( fig.add_subplot(gs[3]) )\n",
    "half_frame(ax[3], 'Event', '$|X_i - X_j|$', font_size = my_font_size)\n",
    "ax[3].bar( np.arange(0,L), abs(random_samples[my_min[3]] - random_samples[my_min[4]]), \n",
    "           color = 'r', alpha = 0.8, width = 0.3 )\n",
    "ax[3].set_ylim(0,4)\n",
    "ax[3].set_xlim(-0.5,L)\n",
    "ax[3].set_xticks(range(0, L, 2))\n",
    "ax[3].hlines([2, 4], -0.5, L+0.5, color = '0.4', lw = 2)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**The pithy statement $-$ many times used as a cudgel $-$ is that correlation is not the same as causation.**  \n",
    "\n",
    "However, this mantra is many times used as a way to dismiss potentially important hypotheses and studies.  While experimentation or randomization of subjects to conditions is a desired way to attempt to exclude the impact of unconsidered factors, either of them is just as robust as their size sample, and actual control of unconsidered factors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Investigate how the sample distribution and sample size affect the number of random samples needed in order to reliably find a pair of highly-correlated samples.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "> 1. Fix the distribution of the samples (keep it to exponential, for example)\n",
    "> \n",
    "> > 2. Select a set of sample sizes for simulation, for example [10, 20, 40, 80, 160]\n",
    "> >\n",
    "> > > 3. Select a number of samples to generate, for example 10 \n",
    "> > > \n",
    "> > > > 4. Generate the necessary sample and calculate the maximum value of the correlation coefficient between all those samples.\n",
    "> > > >\n",
    "> > > > 5. Repeat 4 a few times in order to see whether the maximum value is relatively constant\n",
    "> > > > \n",
    "> > > > 6. Until the maximum correlation coefficient is large, go to 3 and increased the number of samples (for example, by a factor of 2)\n",
    "> > >\n",
    "> > > Repeat with a different sample size\n",
    "> > >\n",
    "> > > Plot number of samples at which you get large correlation coefficients for a given sample size.\n",
    "> >\n",
    "> > Repeat for a different distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's keep going...\n",
    "\n",
    "[Next lesson](nb_12_Estimation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
