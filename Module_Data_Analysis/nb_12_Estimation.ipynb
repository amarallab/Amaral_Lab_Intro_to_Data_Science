{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "With this notebook, we are getting to the practical side of statistics: inference.\n",
    "\n",
    "The big realization to have at this point is that samples are stochastic objects themselves.  This fact takes away  our ability to achieve certainty about the processes under study.  \n",
    "\n",
    "## Words to remember\n",
    "\n",
    "**statistic**\n",
    "\n",
    "**sampling distribution**\n",
    "\n",
    "**point estimator**\n",
    "\n",
    "**point estimate**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T17:28:12.807654Z",
     "start_time": "2023-02-10T17:28:12.010694Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from colorama import Back, Fore, Style\n",
    "from copy import copy, deepcopy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T17:28:15.536430Z",
     "start_time": "2023-02-10T17:28:12.829703Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from random import sample, choice, choices\n",
    "\n",
    "from module_libraries.my_stats import half_frame\n",
    "\n",
    "my_fontsize = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Inference\n",
    "\n",
    "Consider a sample comprised of $N$ i.i.d. random variables $x_1, x_2, ..., x_N$.  (What does i.i.d. mean?)  \n",
    "\n",
    "What is the expected value of $X$? that is, what is the mean of $X$?\n",
    "\n",
    "What we are trying to do here is to **estimate** the value of a **function of** $X$.  When dealing with a stochastic process, we are mostly interested in such estimates.  \n",
    "\n",
    "**THIS ESTIMATE IS ITSELF A RANDOM VARIABLE**.  \n",
    "\n",
    "As such, it also has an expected value and a distribution. The distribution of a function calculated on the random variables comprising a sample is called a **sampling distribution**.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "More generally, any function calculated from the random variables in a sample is called **a statistic**. The mean of a sample of random variables is a statistic.  The difference between the means of two samples of random variables is also a statistic.\n",
    "\n",
    "Sampling distributions are important in statistics because they provide a major simplification *en route* to **statistical inference**. More specifically, they allow analytical considerations to be based on the probability distribution of a statistic, rather than on the joint probability distribution of all the individual sample values. \n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "For example, consider a normal population with mean $\\mu$ and variance $\\sigma^2$. Assume we repeatedly take samples of a given size from this population and calculate the arithmetic mean $\\bar x$  for each sample $-$ **this statistic is called the sample mean**. \n",
    "\n",
    "The distribution of these means is called the **sampling distribution of the sample mean**. \n",
    "\n",
    "This distribution is a normal $N ( \\mu , \\sqrt{\\sigma^2 ~/~ N} )$ since the underlying population is normal but sampling distributions are often close to normal even when the population distribution is not because of the **central limit theorem**.\n",
    "\n",
    "The mean of a sample from a population having a normal distribution is an case where one can get analytical results.  For other statistics and other populations, the situation is more complicated, and we are often unable to obtain closed-form expressions. **In such cases the sampling distributions may be approximated through Monte-Carlo simulations, bootstrap methods, or asymptotic distribution theory**. \n",
    "\n",
    "It is critical that you realzie that the sampling distribution depends on:\n",
    "\n",
    "> the underlying distribution of the population, \n",
    ">\n",
    "> the statistic being considered, \n",
    ">\n",
    "> the sampling procedure employed, and \n",
    ">\n",
    "> the sample size used. \n",
    "\n",
    "**Let's try to check these statements computationally!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling distribution\n",
    "\n",
    "## Monte Carlo simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T17:28:18.269108Z",
     "start_time": "2023-02-10T17:28:18.010647Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "my_sizes = [5, 25, 125]\n",
    "\n",
    "statistic = np.mean\n",
    "name_statistic = 'mean'\n",
    "x_max = 3\n",
    "x = np.linspace(-x_max, x_max, 800 )\n",
    "\n",
    "sampling_dist = {}\n",
    "\n",
    "for n in my_sizes:\n",
    "    sampling_dist[n] = []\n",
    "    for i in range(n_samples):\n",
    "#         sample = stats.expon.rvs(0, 1, size = n)\n",
    "        sample = stats.norm.rvs(mu, sigma, size = n)\n",
    "        sampling_dist[n].append(statistic(sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The cell above has code for generating samples of normally (or exponentially) distributed random variables with sizes 5, 25, 125, and to calculate the sampling distribution.  \n",
    "\n",
    "The current version is set for the **sample mean**. Change the code by **setting the statistic** as the sample's (a) variance, (b) median, and (c) maximum.\n",
    "\n",
    "The code in the cell below enables you to plot the sampling distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T17:28:23.179501Z",
     "start_time": "2023-02-10T17:28:20.879484Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize = (8, 3*3) )\n",
    "\n",
    "ax = []\n",
    "for i, n in enumerate(my_sizes):\n",
    "    ax.append( fig.add_subplot(3,1,i+1) )\n",
    "    if n == my_sizes[1]:\n",
    "        half_frame(ax[1], '', 'Probability density of sampling distribution', \n",
    "                   font_size = my_fontsize)\n",
    "    elif n == my_sizes[2]:\n",
    "        half_frame(ax[-1], f'sample {name_statistic}', '', \n",
    "                   font_size = my_fontsize)\n",
    "    else:\n",
    "        half_frame(ax[-1], '', '', font_size = my_fontsize)\n",
    "    \n",
    "    ax[-1].plot(x, stats.norm.pdf(x, mu, sigma), 'r', lw = 2)\n",
    "    ax[-1].hist(sampling_dist[n], bins = x, density= True, alpha = 0.8)\n",
    "    ax[-1].set_xticks(np.linspace(-x_max, x_max, 5))\n",
    "    ax[-1].set_ylim(0, 6)\n",
    "    ax[-1].set_xlim(-x_max, x_max)\n",
    "\n",
    "    mean_sample_dist = np.mean(sampling_dist[n])\n",
    "    st_dev_sample_dist = np.std(sampling_dist[n])\n",
    "    \n",
    "    # Print useful info\n",
    "    ax[-1].text(x_max+0.05, 4, f'n = {n}', \n",
    "                fontsize = 1.5* my_fontsize, ha = 'right')\n",
    "    ax[-1].vlines([mean_sample_dist-st_dev_sample_dist, \n",
    "                   mean_sample_dist+st_dev_sample_dist], 0, 6, \n",
    "                  color = 'orange', linewidth = 3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Modify the code above so that it can be used to more systematically explore the dependence on the sample size of the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap method\n",
    "\n",
    "The data folder for this module has two files with GPA values for some fictional students. Load the data using pandas.\n",
    "\n",
    "Print the first ten values of the data and find out which column contains the GPAs.\n",
    "\n",
    "Print the last 10 GPA values.\n",
    "\n",
    "Set as your statistic the sample's (a) mean, (b) variance, (c) median, and (d) maximum.\n",
    "\n",
    "From your sample, pick at random with replacement sub-samples with $M = 5, 10, 20$ values. \n",
    "\n",
    "Plot your bootstrapped estimates of the sampling distribution for your chosen statistic.\n",
    "\n",
    "**Hint:** The `random` methods `sample` or `choices` may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-21T18:08:17.082022Z",
     "start_time": "2022-10-21T18:08:16.960185Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T21:03:21.530240Z",
     "start_time": "2022-02-28T21:03:21.523264Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point estimation\n",
    "\n",
    "Point estimation involves the use of sample data to calculate a single value (known as a point estimate or statistic) which is to serve as a **best guess or best estimate** of an unknown **population parameter** (for example, the population mean). More formally, it is **the application of a point estimator to the data to obtain a point estimate**.\n",
    "\n",
    "Point estimation can be used in alternative or in conjunction with **interval estimation**. Interval estimates are typically either **confidence intervals** in the case of frequentist inference, or **credible intervals** in the case of Bayesian inference. \n",
    "\n",
    "Typically, one uses $\\theta$ to denote an **arbitrary statistic**, $\\hat\\Theta$ to denote a **point estimator** for that statistic, and $\\hat\\theta$ to denote a **point estimate** of that statistic.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "Ideally, a point estimator would be unbiased and have low variance (that is, rapid convergence to asymptotic value). These requirements can be quantified using a **loss function** which dependes on the  difference between estimated and true values for an instance of data.  Two examples of loss functions are squared errors and absolute errors. **The risk is the expected value of the loss function**.\n",
    "\n",
    "Because no estimator is optimal for all applications, a number of different \n",
    "estimators have been developed:\n",
    "\n",
    "* method of moments and generalized method of moments\n",
    "* maximum likelihood estimator (MLE)\n",
    "* minimum-variance mean-unbiased estimator (MVUE) $-$ minimizes risk of the squared-error loss-function.\n",
    "* best linear unbiased estimator (BLUE)\n",
    "* minimum mean squared error (MMSE)\n",
    "* median-unbiased estimator $-$ minimizes the risk of the absolute-error loss function\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of moments\n",
    "\n",
    "Let's consider a simple example: a normal distribution. The normal is characterized by two parameters, $\\mu$ and $\\sigma^2$.  If our data was generated by an i.i.d. normal process, then we just need to obtain estimates of these two parameters.  \n",
    "\n",
    "We know from the properties of the normal distribution that $E[X] = \\mu$ and $E[(X-\\mu)^2] = \\sigma^2$.\n",
    "\n",
    "In the method of moments, we calculate the sample moments, $\\bar x$ and $s^2$, and equate them with the parameters:\n",
    "\n",
    "> $~~~~~~~~~ \\bar x = \\hat\\mu ~~~~~~~$ and $~~~~~~~~~ s^2 = \\hat {\\sigma^2}$\n",
    "\n",
    "\n",
    "Things are even easier for the exponential distribution because it has a single parameter $\\lambda$.  In this case, we need to calculate a single moment from the sample.  For the exponential distribution, we have $E[X] = \\tau$. It then follows that:\n",
    "\n",
    "> $~~~~~~~~~ \\bar x = \\hat\\tau$\n",
    "\n",
    "The method of moments is a **consistent estimator**, that is, as the size of the sample increases, the resulting sequence of estimates converges in probability to the true value. This means that **the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated, so that the probability of the estimator being arbitrarily close to its true value converges to one**. \n",
    "\n",
    "Sadly, the method of moments is also typically biased.\n",
    "\n",
    "**Think of what type of situation would result in a bias and code a demonstration of your hypothesis.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "mu = []\n",
    "sigma = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Maximum likelihood estimator (MLE) \n",
    "\n",
    "MLE obtains parameter estimates by finding the parameter values that maximize the likelihood function. The estimates are called maximum likelihood estimates.  From the point of view of Bayesian inference, MLE is a special case of maximum *a posteriori* estimation (MAP) that assumes a uniform prior distribution of the parameters.\n",
    "\n",
    "The likelihood function for a sample of discrete random variable $X$ drawn from a probability distribution with parameters **$\\theta$** is defined as \n",
    "\n",
    "> ${\\cal L}(\\theta; x_1, x2, ..., x_N) = \\Pi_{i=1}^N Pr(X = x_i; \\theta)$\n",
    "\n",
    "The MLE of $\\theta$ is the one that maximizes the likelihood.  Frequently, one maximizes the **log-likelihood instead since it yields the same values but makes calculations easier**.\n",
    "\n",
    "For some models, a maximum likelihood estimator can be found as an explicit function of the observed data. For others, however, no closed-form solution to the maximization problem is known or available, and an MLE can only be found via **numerical optimization**. \n",
    "\n",
    "Moreover, for some problems, there may be multiple values that maximize the likelihood. For other problems, no maximum likelihood estimate exists: either the log-likelihood function increases without ever reaching a supremum value, or the supremum does exist but is outside the set of acceptable parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T19:51:25.345161Z",
     "start_time": "2023-02-07T19:51:25.298993Z"
    }
   },
   "outputs": [],
   "source": [
    "x =  stats.expon.rvs(0, 2, 10000)\n",
    "\n",
    "# print(help(stats.expon.fit))\n",
    "\n",
    "out = stats.norm.fit(x)\n",
    "\n",
    "print(f\"The fit method return a tuple of length {len(out)}\\n\")\n",
    "print(f\"The fit to a normal distribution yields a point estimate for \"\n",
    "      f\"mu of {out[1]:.3f}\\n\")\n",
    "\n",
    "loc, tau = stats.expon.fit(x)\n",
    "\n",
    "my_text = f\"The fit to an exponential distribution yields a point \" + \\\n",
    "          f\"estimate for tau of {tau:.3f}\\n\"\n",
    "print(my_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under very general and non-restrictive conditions, the MLE of a parameter $\\theta$ for a large sample has the following properties:\n",
    "\n",
    "1. $\\Theta_{MLE}$ is an approximately unbiased estimator.\n",
    "\n",
    "2. The variance of $\\Theta_{MLE}$ is **nearly as small** as the variance that could be obtained with **any other estimator**.\n",
    "\n",
    "3. $\\Theta_{MLE}$ has an approximate normal distribution.\n",
    "\n",
    "The implications of these properties are actually quite helpful. Promising *lack of bias* or *smallest variance* is really hard, but the MLE promises the next best thing: that it is close to those goals.\n",
    "\n",
    "The approximate normal distribution is also nice because it tells us we can calculate many things by learning just two parameter values.\n",
    "\n",
    "However, not all is perfect in MLE land.  The implementation of the method may involve using approximations \n",
    "in cases where determining the maximum of the likelihood function is difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does this all mean?\n",
    "\n",
    "Given a random sample, and even if we know what statistical model/process generated that data, we are still facing the challenge of estimating the parameters of the statistical process.\n",
    "\n",
    "As you have seen this is not easy.  The estimates of the parameters are themselves random variables with their own distributions.  \n",
    "\n",
    "What do we **truly know** about our estimates?\n",
    "\n",
    "How can we decide whether two samples were generated by the same statistical model with the same parameter values?\n",
    "\n",
    "How much confidence can we have about anything based on our data?  Imagine the random variable is a  pressure measurement and that we are concerned about a container that will only withstand a given maximum pressure. How confident are we that the container will not fail?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Generate a random sample with N observations (data points) drawn from an exponential distribution.\n",
    "\n",
    "Obtain the MLE estimate of the parameter using the .fit() function.\n",
    "\n",
    "Obtain the method of moments estimate of the parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how the difference in the two estimates changes with N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T22:09:51.713579Z",
     "iopub.status.busy": "2024-09-17T22:09:51.712515Z",
     "iopub.status.idle": "2024-09-17T22:09:51.725070Z",
     "shell.execute_reply": "2024-09-17T22:09:51.723693Z",
     "shell.execute_reply.started": "2024-09-17T22:09:51.713527Z"
    }
   },
   "source": [
    "# Let's keep going...\n",
    "\n",
    "[Next lesson](nb_13_Statistical_intervals.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "196px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
