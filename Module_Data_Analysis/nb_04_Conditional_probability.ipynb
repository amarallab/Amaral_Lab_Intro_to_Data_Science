{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "\n",
    "This notebook provides a brief overview of the interpretations of probability. I also discuss conditional probabilities, and Bayes' theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:31:21.536560Z",
     "start_time": "2023-02-05T21:31:21.472228Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from colorama import Back, Fore, Style\n",
    "from copy import copy, deepcopy\n",
    "from pathlib import Path\n",
    "from sys import path\n",
    "\n",
    "path.append( str(Path.cwd().parent) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:31:36.830477Z",
     "start_time": "2023-02-05T21:31:36.774439Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import scipy.stats as stats\n",
    "\n",
    "from numpy import linspace\n",
    "from scipy import integrate\n",
    "\n",
    "from Amaral_libraries.my_stats import half_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:31:39.287907Z",
     "start_time": "2023-02-05T21:31:39.252020Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fontsize = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional probability\n",
    "\n",
    "Sometimes, probabilities need to be re-evaluated as new information becomes available.  The probability of `SOME CURRENT EVENT` may now be quite different from what it was a week ago.  The necessity to handle these situations gives rise to the concept of conditional probability.  Consider two events $A$ and $B$,\n",
    "\n",
    "> $P(B ~|~ A)$\n",
    "\n",
    "is the probability of $B$ conditional on $A$ being true. The conditional probability obeys the relationship\n",
    "\n",
    "> $P(B ~| ~A) = \\frac{P(A ~\\cap ~B)}{P(A)}$\n",
    "\n",
    "if $P(A) > 0$.\n",
    "\n",
    "From that it follows that\n",
    "\n",
    "> $P(A ~\\cap~ B) = P(B ~|~ A)~ P(A) = P(A ~|~ B)~ P(B)$\n",
    "\n",
    "\n",
    "The definition of conditional probability provides a way to determine whether two events are **independent**. If $P(B ~|~ A) = P(B)$, then $B$ is independent of $A$ (and vice-versa).\n",
    "\n",
    "\n",
    "Let's consider an example that buffles a lot of people.  Imagine that you are a contestant in a game show.  Behind one of three closed doors is a cash prize, behind the others is nothing. You are given the choice of selecting a door to open. After you have selected a door, the host shows you what is behind one of the doors you did not chose, and asks you whether you want to change your choice.  What should you do?\n",
    "\n",
    "To many people there seems like there is no correct answer to this question, that there is a 50% chance that the  cash prize is behind each remaining closed door. However, there is in fact a correct answer.  The host and I both assume that you want to select the door hiding the cash prize, instead of the one with nothing, that is, the host will never choose to open the door hiding the cash prize. Let us also assume that the game is fair, that is, the prize has *a priori* an equal chance of being behind every door.\n",
    "\n",
    "So, let's analyze the problem: Before anything else happens, you have a 1/3 probability of selecting the correct door and a 2/3 probability of selecting the wrong door.\n",
    "\n",
    "Once the host open a door, presumably the one **without** the prize behind it, then things become perfectly clear:\n",
    "\n",
    "> if before you were correct in your original choice, then you should stick with your original choice, \n",
    ">\n",
    "> if before you were wrong in your original choice, then you should change your choice.  \n",
    "\n",
    "**How probable was it that you were correct in your original choice? How probable was it that you were wrong in your original choice?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretations of probability (see [Wikipedia article](https://en.wikipedia.org/wiki/Probability_interpretations)) \n",
    "\n",
    "Mathematically, probability is not controversial and lays on a solid foundation. The interpretation of probability is another matter all together.  Currently, the greatest controversy is between so-called **frequentists** and **Bayesians**.\n",
    "\n",
    "The Wikipedia article mentioned above summarizes the different interpretations:\n",
    "\n",
    "<img src = \"Images/interpretations.png\" width = 100%>\n",
    "\n",
    "In the **classical** interpretation, one assumes that basic events are equi-probable $-$ the principle of indifference.  The conceptual basis for this is an hypothesized symmetry of all possible outcomes. For example, when considering the process of rolling a die, the assumption is that all 6 outcomes are equally probable, so the probability of each outcome is simply 1/6. In general, if there are $N$ outcomes, then the probability of each outcome is\n",
    "\n",
    "> $p = \\frac{1}{N}$.\n",
    "\n",
    "A major advantage of this approach is that it **does not require past observations**.\n",
    "\n",
    "Clearly, this will interpretation will fail when there are an infinite number of outcomes. Another major problem is that in case they are not equi-probable, then their probabilities need to be determined somehow.  The classical approach is thus well suited to fair coins and dice, but not to unfair ones. It is also not appropriate for situations such as determining **the probability that tomorrow's maximum temperature will be greater than 40$^o$F**.\n",
    "\n",
    "In the **frequentist** interpretation, one uses past observations to determine the probability of an event $-$ the probability of an event is determined by how frequently it occurs.  That it, probability values are obtained empirically.  It follows that empirical basis of the frequentist approach meshes well with the scientific method and the search for 'truth'. \n",
    "\n",
    "Frequentists determine that the probability of getting `heads` in a coin toss is 1/2, not because there are two equally likely outcomes but because repeated series of large numbers of trials demonstrate that the empirical frequency converges to the limit 1/2 as the number of trials goes to infinity.  This results in the frequentist approach's two major weaknesses. First, it requires lots of past observations in order to estimate probabilities. Even then, for any two finite series of observations, even very long ones, the estimated probability will be somewhat different. The second weakness derives from the first. In order to determine that the probability of tossing `heads` is 1/2, is has to make use of the theory of errors, which makes statements about the probability of observing a given error. This results in a circular process: the concept of probability needs the concept of frequency but the concept of frequency needs the concept of probability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **Bayesian** interpretation $-$ which is also denoted **subjective** or **epistemic** $-$ probability is a measure of the **degree of belief** of an individual or organization when assessing an uncertain situation. As such, two individuals may view the occurrence of the same event as having different probabilities.  If I am playing a dice game with you, I may believe the dice to be fair while you may believe that they are loaded. \n",
    "\n",
    "More generally, the world around us shows us many examples of individuals holding different beliefs.  Stock traders make buying/selling decisions based on their beliefs about where a stock's price is headed. Pundits argue for different outcomes in an election. Different models provide different estimates for what the global temperature will be in 2050.\n",
    "\n",
    "While this makes it sound as if all beliefs $-$ opinions $-$ are of equal value, this is not correct.  As we will see below, Bayesians use Bayes' theorem to define a rigorous process for using data for updating one's belief's. The challenge comes from the fact that this process relies on the definition of a prior probability.  For example, when considering the tossing of a coin, **in the absence of any data**, a Bayesian could, for example, use the principle of indifference to select the prior probability of tossing `heads` (see analysis below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T19:33:30.622716Z",
     "start_time": "2023-01-05T19:33:30.500982Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Fore.RED, Style.BRIGHT)\n",
    "print('Two acceptable priors for the probability of tossing heads:\\n', \n",
    "      Style.RESET_ALL)\n",
    "\n",
    "x = linspace(0, 1, num = 100)\n",
    "\n",
    "fig = plt.figure( figsize = (6, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "half_frame(ax, 'p', 'Prior probability', font_size= my_fontsize)\n",
    "\n",
    "ax.plot(x, stats.beta(5, 5).pdf(x))\n",
    "ax.plot(x, stats.beta(2, 2).pdf(x))\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually gives rise to one of the weaknesses of the Bayesian interpretation.  The issue is that for a given problem, multiple thought experiments could apply, and choosing one is a matter of judgement: different people may assign different prior probabilities. This issue is known as the **reference class problem**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Theorem\n",
    "\n",
    "The concept of conditional probability connects belief (given by probability) with information.  This has actually enormous consequences.  Since one cannot ever observe an infinite number of events, one cannot in most situations truly determine $P(E)$.  One can nonetheless build hypotheses for what $P(E)$ is $-$ a so-called **prior**.  **Conditional probabilities enable us to update our priors as new information becomes available!**\n",
    "\n",
    "This is expressed by Bayes' Theorem which appear to simply re-write an equation above but does so much more\n",
    "\n",
    "\n",
    "> $P(B | A) = \\frac{P(A | B)~ P(B)}{P(A)}$\n",
    "\n",
    "if $P(A) > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make use of Bayes' theorem to understand how new data enables us to update our prior probability for $p_{\\rm heads}$. Let's denote the new data by ${\\bf X} = \\{X_1, ..., X_m\\}$, the current estimate of our confidence in the value of $p_{\\rm heads}$ by $P_{\\theta}(p_{\\rm heads})$. Then, it follows that the **posterior probability** of $P'(p_{\\rm heads})$ is given by:\n",
    "\n",
    "> $P'_{\\theta}(p_{\\rm heads}) = P(p_{\\rm heads} ~|~ {\\bf X}) = \\frac{P({\\bf X}~|~ p_{\\rm heads})}{P({\\bf X})}~P_{\\theta}(p_{\\rm heads})$ .\n",
    "\n",
    "Things get computationally complex because:\n",
    "\n",
    "> $P({\\bf X}) = \\int_0^1 ~P({\\bf X}~|~ p_{\\rm heads})~P_{\\theta}(p_{\\rm heads}) ~dp_{\\rm heads} $.\n",
    "\n",
    "To demonstrate the process, let us first define our initial prior and generate a set of observation by simulating the tossing of a coin in the computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:35:05.046935Z",
     "start_time": "2023-02-05T21:35:04.993022Z"
    }
   },
   "outputs": [],
   "source": [
    "n_observations = 1000\n",
    "X = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:35:20.790322Z",
     "start_time": "2023-02-05T21:35:20.722639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate fair observations\n",
    "#\n",
    "X['fair'] = stats.randint.rvs(0, 2, size = n_observations)\n",
    "print(f\"The observed probability of tossing heads is {sum(X['fair'])/len(X['fair']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:35:59.343311Z",
     "start_time": "2023-02-05T21:35:59.293562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate quite biased observations\n",
    "#\n",
    "bias = 0.6\n",
    "X['biased'] = [1 if y < bias else 0 for y in stats.uniform.rvs(size = n_observations)]\n",
    "print(f\"The observed probability of tossing heads is {sum(X['biased'])/len(X['biased']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:36:48.179566Z",
     "start_time": "2023-02-05T21:36:48.131551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate very biased observations\n",
    "#\n",
    "bias = 0.8\n",
    "X['very biased'] = [1 if y < bias else 0 for y in stats.uniform.rvs(size = n_observations)]\n",
    "print(f\"The observed probability of tossing heads is \"\n",
    "      f\"{sum(X['very biased'])/len(X['very biased']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:38:21.762171Z",
     "start_time": "2023-02-05T21:38:21.709772Z"
    }
   },
   "outputs": [],
   "source": [
    "p_heads = linspace(0, 1, num = 1+2**7)\n",
    "initial_prior = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:38:22.272208Z",
     "start_time": "2023-02-05T21:38:22.223506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define priors 1\n",
    "#\n",
    "alpha, beta = 5, 5\n",
    "initial_prior['beta55'] = stats.beta(alpha, beta).pdf(p_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:38:22.787704Z",
     "start_time": "2023-02-05T21:38:22.738559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define prior 2\n",
    "#\n",
    "initial_prior['uniform01'] = stats.uniform(0, 1).pdf(p_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:38:23.344146Z",
     "start_time": "2023-02-05T21:38:23.289839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define (unreasonable) prior 3\n",
    "#\n",
    "initial_prior['uniform.4.2'] = stats.uniform(0.4, 0.2).pdf(p_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the probability of observing the data given the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:38:28.131021Z",
     "start_time": "2023-02-05T21:38:28.072388Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_prob_data(prior, data, p_heads):\n",
    "    \"\"\"\n",
    "    Calculates the probability of observing the data given the prior. \n",
    "    Integrates prior distribution multiplied by the probability of observing\n",
    "    the data given that parameter value using the romb algorithm which \n",
    "    requires a list of 1+2^k equally spaced parameter values and the value \n",
    "    of the prior at those values\n",
    "    \n",
    "    inputs:\n",
    "        prior -- list of float, values of the prior at different values of parameter\n",
    "        data -- float, value of observation\n",
    "        p_heads -- kust of floats, different values of parameter\n",
    "        \n",
    "    outputs:\n",
    "        float, probability of observing the data given the prior\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    integrand = [p*p_theta if data == 1  else (1-p)*p_theta \n",
    "                 for p, p_theta in zip(p_heads, prior)]\n",
    "    \n",
    "    dx = 1 / len(integrand)\n",
    "    return integrate.romb(integrand) * dx\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with the different priors and levels of bias in the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T21:40:11.828991Z",
     "start_time": "2023-02-05T21:40:11.483549Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize = (8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "prior = copy(initial_prior['beta55'])\n",
    "for i, data in enumerate( X['fair'] ):\n",
    "    prob_data = calculate_prob_data(prior, data, p_heads)\n",
    "    \n",
    "    posterior = []\n",
    "    for p, p_theta in zip(p_heads, prior):\n",
    "        \n",
    "        if data == 1:\n",
    "            posterior.append(p * p_theta / prob_data)\n",
    "        else:\n",
    "            posterior.append((1-p) * p_theta / prob_data)\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        ax.plot(p_heads, posterior, 'r', alpha = 0.4)\n",
    "    prior = copy(posterior)\n",
    "\n",
    "# Plot initial and final form of P(p_heads)\n",
    "#\n",
    "half_frame(ax, '$p_{heads}$', 'Posterior probability')\n",
    "ax.plot(p_heads, initial_prior['beta55'], lw = 4)    \n",
    "ax.semilogy(p_heads, posterior, 'r', lw = 4)\n",
    "# ax.plot(p_heads, posterior, 'r', lw = 4)\n",
    "\n",
    "ax.vlines([0.2, 0.4, 0.6, 0.8], 0, max(posterior)*10, color = '0.6')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(10**(-10), max(posterior)*10)\n",
    "\n",
    "# Check integral of final posterior\n",
    "\n",
    "final = integrate.romb(posterior) / len(posterior)\n",
    "print(Fore.RED, Style.BRIGHT)\n",
    "print(f\"The integral of the final posterior estimate equals {final:.6f}\\n\", \n",
    "      Style.RESET_ALL)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical impact of Bayes' theorem\n",
    "\n",
    "One of the most important applications of Bayes' theorem is in determining whether mass testing for a medical condition is appropriate or not. Consider the following situation involving a low prevalence infectious disease such as HIV. Assume that the incidence rate in the population is 0.1%, \n",
    "\n",
    "> $P(D) = 0.001$ .  \n",
    "\n",
    "It follows that the probability of not having the disease is\n",
    "\n",
    "> $P( \\not D) = 0.999$ .\n",
    "\n",
    "Consider a nearly perfect test that **correctly diagnosis the disease** 99% of the time. Thus, the probability of getting a `+` result in the test of an individual who has the disease is:\n",
    "\n",
    "> $P(+~|~D) = 0.99$ . \n",
    "\n",
    "Assume also that the test **correctly diagnosis absence of the disease** 95% of the time. Thus, the probability of getting a `-` result in the test of an individual who does not have the disease is: \n",
    "\n",
    "> $P(- ~|~ \\not D) = 0.95$,\n",
    "\n",
    "and it follows that \n",
    "\n",
    "> $P(+~|~ \\not D) = 0.05$.\n",
    "\n",
    "\n",
    "**Imagine you take the test and get a positive result. In the absence of any other information, what is the probability that you *do* have the disease?**\n",
    "\n",
    "> $P(D~ |~ +) = $ ?\n",
    "\n",
    "Let's 'unpack' this probability\n",
    "\n",
    "> $P(D~ |~ +) = \\frac{P(+ ~|~ D)~ P(D)}{P(+)}$ \n",
    "\n",
    "> $~~~~~~~~~~~~~~~~  = \\frac{(0.99 * 0.001)}{P(+)}$\n",
    "\n",
    "> $~~~~~~~~~~~~~~~~  = \\frac{0.00099}{P(+)}$\n",
    "\n",
    "We still need to determine $P(+)$.\n",
    "\n",
    "> $P(+) = P(+ ~|~ D)~ P(D) + P(+~|~ \\not D )~ P(\\not D)$\n",
    "\n",
    "> $~~~~~~~~  = (0.99 * 0.001) + (0.05 * 0.999) = 0.05094$\n",
    "\n",
    "and it follows that \n",
    "\n",
    "> $P(D~ |~ +) = 0.0194$\n",
    "\n",
    "**Less than 2%!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you should get another test.  \n",
    "\n",
    "What does it mean if you get a positive result again?\n",
    "\n",
    "Why would that change things?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
