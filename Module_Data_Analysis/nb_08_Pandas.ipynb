{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "In this unit we will learn the basics of analyzing structured data. In the process we will cover:\n",
    "\n",
    "* What is structured data\n",
    "* How to use Pandas to read and write structured data\n",
    "* Basic indexing operations of Pandas\n",
    "* Basic operations (math and plotting) with Pandas\n",
    "* Handling dates and times\n",
    "* The 'split-apply-combine' framework for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T20:02:13.284007Z",
     "start_time": "2023-01-05T20:02:12.754832Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from colorama import Back, Fore, Style\n",
    "from pathlib import Path\n",
    "from sys import path\n",
    "\n",
    "path.append( str(Path.cwd().parent) )\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T20:02:13.338187Z",
     "start_time": "2023-01-05T20:02:13.320953Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T20:02:17.277901Z",
     "start_time": "2023-01-05T20:02:16.608300Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Pandas\n",
    "\n",
    "Pandas is a contraction for \"panel data\", such as what you create when using a spreadsheet. Pandas can do a lot, so I find it easier to think of it as this for data:\n",
    "\n",
    "![party](http://cdn.protoolreviews.com/wp-content/uploads/ptr/4433.jpg)\n",
    "\n",
    "But first let's confront the ugly reality.\n",
    "\n",
    "## Pandas is not very Pythonic\n",
    "\n",
    "Actually **using** Pandas and not just using it to read files can be conceptually difficult and is a bit of a mental switch compared to most of what we have learned so far. If you want to iterate over things, you can't use a `for` loop easily. Instead you'll need to use specific Pandas methods to do whatever functions you want. These little differences add up and can wear on you, which might make you want to stop using Pandas. That's a fine way to feel (you don't really *have* to use it), but there are some big benefits to using it, that for a lot of people, are worth the costs.\n",
    "\n",
    "## Benefits of Pandas\n",
    "\n",
    "1. Pandas handles a lot of file I/O drudgery for you. I'll show you this in a bit, but reading CSV files and accessing data in them is super simple\n",
    "2. Pandas has a lot of *magic* built into, automaticallly taking care of many type conversions after reading a file\n",
    "3. Using Pandas **is** like working with [SQL](https://en.wikipedia.org/wiki/SQL) (don't know what SQL is? Don't worry, it's a bit advanced for this course but is surely something you'll encounter if you continue to program so it's worth reading up on). So learning Pandas means that you'll have a good idea of the underpinnings how SQL databases work which might help you later in your programming education (although the syntax is different).\n",
    "\n",
    "If you like using or want to continue using Pandas here is some recommended additional reading\n",
    "\n",
    "The Pandas tutorial pages http://pandas.pydata.org/pandas-docs/stable/tutorials.html\n",
    "\n",
    "10 minutes to Pandas http://pandas.pydata.org/pandas-docs/stable/10min.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures in Pandas\n",
    "\n",
    "First, let's show some of this `automagic`. In the `Data` folder there is a folder named `College-Majors` that contains a number of files with information on career outcomes for graduates from a large collection of college majors.\n",
    "\n",
    "This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T20:02:20.388111Z",
     "start_time": "2023-01-05T20:02:19.249186Z"
    }
   },
   "outputs": [],
   "source": [
    "cat Data/College-Majors/recent_Arts.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load one of those files right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T20:02:21.437113Z",
     "start_time": "2023-01-05T20:02:21.373318Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = Path.cwd() / 'Data'\n",
    "college_folder = data_folder / 'College-Majors'\n",
    "\n",
    "arts_file = college_folder / 'recent_Arts.csv'\n",
    "pd.read_csv( arts_file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Pandas reads in a `CSV` it turns it into its own data structure called a `DataFrame`. This `DataFrame` is actually a Python class, you can think of it as just a type of *object*. Our data is inside this *object* and it controls how we can interact with it (so you can see the first difference between this and regular programming).\n",
    "\n",
    "(The nice formatting that makes it look like an Excel spreadsheet is provided by the `Jupyter Notebook`!)\n",
    "\n",
    "Now, let's actually store the data `CSV` so we can operate on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:36:30.959166Z",
     "start_time": "2022-12-26T20:36:30.911805Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(arts_file)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics of a DataFrame\n",
    "\n",
    "Since the object we're working with is a `DataFrame` you'll very frequently see people assign it to a variable named `df`. \n",
    "\n",
    "Specifying a value in a `DataFrame` requires two coordinates: a *column* and an *index*.\n",
    "\n",
    "The **columns** run across the **top**\n",
    "\n",
    "The **indices** run down the **left** (for now, you can think of these as rows)\n",
    "\n",
    "We can get see the possible values for these coordinates by calling them by name from the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:36:37.978086Z",
     "start_time": "2022-12-26T20:36:37.936421Z"
    }
   },
   "outputs": [],
   "source": [
    "#The columns are the labels across the top\n",
    "print( df.columns ) \n",
    "print()\n",
    "\n",
    "#The indexes run down the side\n",
    "print( df.index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data in Pandas\n",
    "\n",
    "Pandas supports two approaches for accessing data that is stored in a **column**. \n",
    "\n",
    "One approach is the `.attribute` of object (think `string.whitespace`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:36:41.445286Z",
     "start_time": "2022-12-26T20:36:41.397678Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(df.Major))\n",
    "print()\n",
    "df.Major"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Notice that the output of using the `.attribute` approach is a different kind of object: a `Series`.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The other approach is the `key` in `dictionary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:36:57.985974Z",
     "start_time": "2022-12-26T20:36:57.939916Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(df['Major']))\n",
    "print()\n",
    "df['Major']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:36:58.540682Z",
     "start_time": "2022-12-26T20:36:58.490302Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Major'] == df.Major"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You can see that these two approaches are quite interchangeable.  However, the attribute approach is difficult to use if column names include spaces. That is one reason to name columns using `_` instead of ` `.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "In order to access the information from specific 'rows', we must make use of the **indices**. \n",
    "\n",
    "`DataFrame` indices can be treated quite similar to the `indices` in a `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:37:14.834077Z",
     "start_time": "2022-12-26T20:37:14.790592Z"
    }
   },
   "outputs": [],
   "source": [
    "print( f\"The first value in the 'Major' column is {df['Major'][0]}\\n\" )\n",
    "\n",
    "print( f\"The second value in the 'Major' column is {df.Major[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing in Pandas\n",
    "\n",
    "We can also access **slices of the rows** in a `dataframe` by using the syntax introduced for `lists`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:37:16.336486Z",
     "start_time": "2022-12-26T20:37:16.289514Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df['Major'][0::2] )\n",
    "print()\n",
    "print(df.Major[6::-2])\n",
    "print()\n",
    "print( df[0::2]['Major'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "You may also wish to **slice on columns**...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-12T16:47:37.155635Z",
     "start_time": "2022-09-12T16:47:36.334072Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df['Major':'Total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nah! That does not work!!!**\n",
    "\n",
    "We need a method based on labels in order to slice along columns.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### `.loc[]`\n",
    "\n",
    "The `.loc[]` approach is intended to work on labels.  \n",
    "\n",
    "**Note that when you are using this approach you have to specify slices for both rows and columns.**\n",
    "\n",
    "And the slices are placed within a single set of `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T19:30:31.679336Z",
     "start_time": "2022-09-20T19:30:31.643999Z"
    }
   },
   "outputs": [],
   "source": [
    "print( type(df.loc[:, 'Major':'Total']) )\n",
    "print()\n",
    "\n",
    "print( df.loc[:, 'Major':'Total'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Notice that the order of the labels matter.  \n",
    "\n",
    "**Indices must came before column names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T19:30:51.467816Z",
     "start_time": "2022-09-20T19:30:51.436456Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df.loc['Major'::4, ::2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And `.loc[]` really wants to see two sets of labels separated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:38:05.743248Z",
     "start_time": "2022-12-26T20:38:05.549205Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Total.loc[0] =  5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:38:06.093730Z",
     "start_time": "2022-12-26T20:38:06.043445Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Total.loc[0] =  3340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can use `.loc[]` with a single set of labels on a `Series` though "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T19:34:30.909261Z",
     "start_time": "2022-09-20T19:34:30.874447Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df['Major'].loc[::2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "but we cannot chain `.loc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T19:34:34.395649Z",
     "start_time": "2022-09-20T19:34:34.364295Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df.loc['Major'::4].loc[::2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### `.iloc[]`\n",
    "\n",
    "The `.iloc[]` approach is integer based and works on the indices.\n",
    "\n",
    "Notice that 'Major' is the third column. Then, it follows that...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T19:36:09.260982Z",
     "start_time": "2022-09-20T19:36:09.228905Z"
    }
   },
   "outputs": [],
   "source": [
    "print( type(df.iloc[:, 2::8]) )\n",
    "print()\n",
    "\n",
    "print( df.iloc[:, 2::8] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "To gain a better understanding of how `.iloc` work, let's sort the `dataframe` so that the `index` labels are **no longer referring to the order of the rows.**\n",
    "\n",
    "We start by sorting the rows of the `dataframe` by the values in the **Total** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:38:44.518911Z",
     "start_time": "2022-12-26T20:38:44.466554Z"
    }
   },
   "outputs": [],
   "source": [
    "df_total_sorted = df.sort_values('Total', ascending = False)\n",
    "df_total_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice how the numbers in the first (unnamed) column are not in order!**\n",
    "\n",
    "When we use the `.iloc` method on `df_total_sorted` we retrieve the specified rows in this new `dataframe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:38:50.094366Z",
     "start_time": "2022-12-26T20:38:50.048249Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df[3:5]['Major'] )\n",
    "print()\n",
    "\n",
    "print( df_total_sorted[3:5]['Major'] )\n",
    "print()\n",
    "\n",
    "print( df_total_sorted['Major'][3:5] )\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T16:30:30.395041Z",
     "start_time": "2022-07-08T16:30:30.278239Z"
    }
   },
   "source": [
    "Notice how when we use the `key` in `dictionary` approach for column names **the order of the `[]` does not matter.**\n",
    "\n",
    "In contrast, when using the `iloc[]` approach, the order of the indices inside the `[]` matters.  The first one refers to the rows and the second one refers to the columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:38:55.728690Z",
     "start_time": "2022-12-26T20:38:55.681743Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df_total_sorted.iloc[3:5, 2])\n",
    "print()\n",
    "\n",
    "print( df_total_sorted.iloc[2, 3:5])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And using the column names does not work. We have to use an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:39:06.783165Z",
     "start_time": "2022-12-26T20:39:06.159445Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df_total_sorted.iloc[3:5, 'Major'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "And you also cannot use `iloc[]` with a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:39:18.223594Z",
     "start_time": "2022-12-26T20:39:17.068390Z"
    }
   },
   "outputs": [],
   "source": [
    "df_total_sorted[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "If we call `.loc[]` on `df_total_sorted` we see that it filters by the indices defined in `df` but not in the manner one could expect.  **These are now looked up as a label, not an index/order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:39:29.432689Z",
     "start_time": "2022-12-26T20:39:29.382866Z"
    }
   },
   "outputs": [],
   "source": [
    "print( df_total_sorted.loc[3:5, 'Major'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Series`: Pandas' other data type\n",
    "\n",
    "You will recall that earlier on when we retrieved the values from a single column, `pandas` returned a `Series` object.\n",
    "\n",
    "We can create `Series` by pulling a single column or a single row from a `dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:39:41.553477Z",
     "start_time": "2022-12-26T20:39:41.507206Z"
    }
   },
   "outputs": [],
   "source": [
    "print('---- Single column\\n')\n",
    "serie_1 = df['Major']\n",
    "print(serie_1)\n",
    "\n",
    "print('\\n\\n---- Single row\\n')\n",
    "serie_2= df_total_sorted.loc[3]\n",
    "print(serie_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "Many of the methods defined on `dataframes` also work on `Series` as long you account for the fact that there is only one column, so you do not need a label for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:39:52.941324Z",
     "start_time": "2022-12-26T20:39:52.851680Z"
    }
   },
   "outputs": [],
   "source": [
    "print( serie_1[0:6:2] )\n",
    "print()\n",
    "\n",
    "print( serie_2[0:6:2] )\n",
    "print()\n",
    "\n",
    "print( serie_2['Major':'Total'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The benefits of using `pandas`\n",
    "\n",
    "Pandas has a LOT of cool functionality. It can read (and write) `.xls`/`.xlsx` files! \n",
    "\n",
    "**Now you don't need to open a workbook in Excel and save it to a `CSV` when someone sends them to you!**\n",
    "\n",
    "When we read an Excel spreadsheet, all we have to say is what sheet we want to use in the file. You can use either the sheet_name (if it has one) or just give it the index of the sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:40:00.570391Z",
     "start_time": "2022-12-26T20:40:00.272825Z"
    }
   },
   "outputs": [],
   "source": [
    "df_arts = pd.read_excel( college_folder / 'recent_Arts.xlsx', sheet_name = 0)\n",
    "df_arts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "If you want to know how many sheets there are in the *Excel* workbook, you can use `.ExcelFile()` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:40:36.065684Z",
     "start_time": "2022-12-26T20:40:36.012707Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.ExcelFile( college_folder / 'recent_Arts.xlsx' ) as reader:\n",
    "    line = ''\n",
    "    for name in reader.sheet_names:\n",
    "        line += f\"\\t{name}\\n\" \n",
    "    print(f\"This excel file has {len(reader.sheet_names)} worksheets.\\n\"\n",
    "          f\"Their names are:\\n{line}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's not just *Excel*. `Pandas` can actually read/write a large number of different and really useful file formats that can be essential when working with collaborators who might not be quite so `Python` inclined:\n",
    "\n",
    "> read_clipboard\n",
    ">\n",
    "> read_csv\n",
    ">\n",
    "> read_excel\n",
    ">\n",
    "> read_feather\n",
    ">\n",
    "> read_fwf\n",
    ">\n",
    "> read_gbq\n",
    ">\n",
    "> read_hdf\n",
    ">\n",
    "> read_html\n",
    ">\n",
    "> read_json\n",
    "> \n",
    "> read_orc\n",
    ">\n",
    "> read_parquet\n",
    ">\n",
    "> read_pickle\n",
    ">\n",
    "> read_sas\n",
    ">\n",
    "> read_spss\n",
    ">\n",
    "> read_sql\n",
    ">\n",
    "> read_sql_query\n",
    ">\n",
    "> read_sql_table\n",
    ">\n",
    "> read_stata\n",
    ">\n",
    "> read_table\n",
    ">\n",
    "> read_xml\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Pandas helps you quickly explore and manipulate data as you learn about your dataset basics. One quick benefit is the built-in plotting directly from the dataframe.\n",
    "\n",
    "Let's say that we wanted to make a plot that examined the difference between the majors in terms of the raw employment numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:43:09.343342Z",
     "start_time": "2022-12-26T20:43:09.293695Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:43:18.358899Z",
     "start_time": "2022-12-26T20:43:18.227132Z"
    }
   },
   "outputs": [],
   "source": [
    "df_arts['Employed'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it's pretty ugly, but it was also fast and easy! Note that the `;` suppresses `matplotlib` print statements\n",
    "\n",
    "Let's actually think about this for a moment because something magical just happened and we all probably took it for granted.\n",
    "\n",
    "**We just plotted numeric data from the file that I read in with a single command.**\n",
    "\n",
    "**When did I change the type of that data to be an integer so that we could plot it??**\n",
    "\n",
    "You might recall when we read files in the past using `open('super_cool_file.csv')`, **everything was read as a string by default, even numbers!**\n",
    "\n",
    "When we load data with Pandas it automatically converted the 'Employed' column data to integers. In fact, Pandas does this with all of the columns and when it does this it picks the **least** expansive data type that **accommodates all the data in the column**.\n",
    "\n",
    "We can check this, so the `Unemployed` column should be integers also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:43:25.524459Z",
     "start_time": "2022-12-26T20:43:25.476721Z"
    }
   },
   "outputs": [],
   "source": [
    "df_arts['Unemployed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it is actually `int64` which is the data format used by `numpy`.\n",
    "\n",
    "Following on this success, we can expect the `ShareWomen` column to contain `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:43:30.794264Z",
     "start_time": "2022-12-26T20:43:30.743583Z"
    }
   },
   "outputs": [],
   "source": [
    "df_arts['ShareWomen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to be aware of though, is that if we had a single string in that column of data **none of it would be converted**. \n",
    "\n",
    "All of the read values would be strings because any number can represented as a string, just as text data can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "**Getting back to our goal of plotting the data.**\n",
    "\n",
    "A bar plot is more appropriate for this data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:43:49.402627Z",
     "start_time": "2022-12-26T20:43:49.263690Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Employed'].plot(kind = 'bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! That's a little bit better!\n",
    "\n",
    "But we should never have a graph without a y-label! To change labels we'll need to operate on the `matplotlib` object. \n",
    "\n",
    "**Applying the method `plot()` to a `DataFrame` returns a `Matplotlib` axis object.**\n",
    "\n",
    "We can then use what we know about `Matplotlib` to customize the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:44:30.317889Z",
     "start_time": "2022-12-26T20:44:30.167671Z"
    }
   },
   "outputs": [],
   "source": [
    "# I'm changing the color of the bars!\n",
    "#\n",
    "ax = df['Employed'].plot(kind = 'bar', color = 'darkred')\n",
    "\n",
    "# Now I can set the y-axis label\n",
    "#\n",
    "ax.set_ylabel('Students Employed')\n",
    "\n",
    "# I can also set the xticks to the major names. And \n",
    "#\n",
    "major_labels = df['Major']\n",
    "ax.set_xticklabels(major_labels, rotation = 45, ha = 'right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical operations\n",
    "\n",
    "Pandas has methods implementing numerous mathematical operations built directly into the `dataframe` object using `numpy`. \n",
    "\n",
    "For exampple, if you want to know the average of a column's value or the number of rows with entries (not every position has to have a value!), you can accomplish that in a straightforward manner.\n",
    "\n",
    "Let's start by just counting the number of values in each column (it should be 8 in every column since every spot in our spreadsheet was filled out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:44:35.481059Z",
     "start_time": "2022-12-26T20:44:35.426883Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df['Employed'].count())\n",
    "print()\n",
    "\n",
    "df['Employed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:44:39.008827Z",
     "start_time": "2022-12-26T20:44:38.956789Z"
    }
   },
   "outputs": [],
   "source": [
    "help(df.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the documentation that\n",
    "\n",
    ">Count non-NA cells for each column or row.\n",
    ">    \n",
    ">    The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\n",
    "    on `pandas.options.mode.use_inf_as_na`) are considered NA.\n",
    "\n",
    "\n",
    "When there is a missing value in the raw data `pandas` replaces that value with a `Not a Number` or `NaN`.\n",
    "\n",
    "We can learn about what this means by creating a new column with no values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:44:45.700119Z",
     "start_time": "2022-12-26T20:44:45.649521Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Test_column'] = np.nan\n",
    "df.loc[:, 'College_jobs':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we count, it won't be the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:44:50.731700Z",
     "start_time": "2022-12-26T20:44:50.678618Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Test_column'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count values for the entire `dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:44:53.479022Z",
     "start_time": "2022-12-26T20:44:53.422268Z"
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can also just count a few columns or single column by chaining the `.count()` method after we slice the `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T20:13:35.649428Z",
     "start_time": "2022-09-20T20:13:35.609479Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[0:2].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "There are other useful functions built in too. We can quickly take the mean or median of a column also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:45:37.047585Z",
     "start_time": "2022-12-26T20:45:36.990270Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Employed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T20:13:43.581762Z",
     "start_time": "2022-09-20T20:13:43.545181Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Employed.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can even get the mean, medians, and a host of other summary statistics for all columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:45:16.240592Z",
     "start_time": "2022-12-26T20:45:16.174357Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Employed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As you would expect, the `.describe()` method returns a `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T20:14:01.380317Z",
     "start_time": "2022-09-20T20:14:01.339119Z"
    }
   },
   "outputs": [],
   "source": [
    "print( type( df.Employed.describe() ) )\n",
    "print()\n",
    "\n",
    "df.Employed.describe()['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thus, it can be accessed in the usual ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:45:49.752151Z",
     "start_time": "2022-12-26T20:45:49.700540Z"
    }
   },
   "outputs": [],
   "source": [
    "my_column = 'Men'\n",
    "\n",
    "print(df[my_column].describe()['50%'])\n",
    "print()\n",
    "\n",
    "print(df[my_column].describe()[5])\n",
    "print()\n",
    "\n",
    "print(f\"The median of number of {my_column} is {df[my_column].describe()['50%']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy \"parallelization\" of  code\n",
    "\n",
    "Just as with `numpy`, `pandas` enable us to easily parallelize operations on columns of data. \n",
    "\n",
    "Note that the data types with a single row are unlikely to be identical, so the similar approach for rows would be unlike to work.\n",
    "\n",
    "You can easily implement operations across all values in a column. For example, you can divide the values in a column by the corresponding values in another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:45:54.632554Z",
     "start_time": "2022-12-26T20:45:54.583928Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Employed']/df['Total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save the outcome of this operation, you assign it to another variable or to a new column in the `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:46:00.227144Z",
     "start_time": "2022-12-26T20:46:00.169596Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Percent_Employed'] = 100 * df.Employed / df.Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.head()` method to print the top few rows of the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:46:06.578821Z",
     "start_time": "2022-12-26T20:46:06.530785Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'College_jobs':].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br>\n",
    "\n",
    "And you can perform more elaborate calculations too... the z-score is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:46:16.577125Z",
     "start_time": "2022-12-26T20:46:16.527970Z"
    }
   },
   "outputs": [],
   "source": [
    "df['z_score_college_jobs'] = ( (df.College_jobs - df.College_jobs.mean()) / \n",
    "                                df.College_jobs.std() )\n",
    "\n",
    "df['z_score_college_jobs'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "A great strength of Pandas is the ability to query a `dataframe` and extract only the rows or columns that meet some criteria.\n",
    "\n",
    "For example, imagine you want to focus on those majors with more than 20,000 graduates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:46:20.777602Z",
     "start_time": "2022-12-26T20:46:20.729782Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following statement parallelizes the comparison of the values in the column *Total* to the value 20,000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:46:25.210187Z",
     "start_time": "2022-12-26T20:46:25.162775Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Total > 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Boolean `Series` can be used to filter the appropriate rows in the `dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:46:30.169590Z",
     "start_time": "2022-12-26T20:46:30.114003Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.Total > 20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where things get annoying.  `pandas` developers chose, for convenience, I imagine, to settle on a distinct set of symbols for the logical operator:\n",
    "\n",
    "> `|` stands for `or`\n",
    ">\n",
    ">  `&` stands for `and`\n",
    ">\n",
    "> `~` stands for `not`\n",
    "\n",
    "And notice that **every individual logical operation must be placed inside parentheses**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:46:59.550251Z",
     "start_time": "2022-12-26T20:46:59.498871Z"
    }
   },
   "outputs": [],
   "source": [
    "df[(df.Total > 20000) | (df.Major_code < 6003)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:47:00.323184Z",
     "start_time": "2022-12-26T20:47:00.260308Z"
    }
   },
   "outputs": [],
   "source": [
    "df[(df.Total > 20000) & (df.Major_code < 6003)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain selection operators off of the query also if we want to know a bit more about the resulting column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:47:02.162226Z",
     "start_time": "2022-12-26T20:47:02.114895Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.Employed > 20000].Unemployment_rate.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "Querying is actually a sort of parallelization. It enables us to very compactly tell the computer what operations to implement across all rows or columns. The masks returned by queries could be easily implemented with `for` loops\n",
    "\n",
    "    for total_students in total:\n",
    "        if total_students > 20000:\n",
    "            #Continue with code\n",
    "            \n",
    "However, a large set of indented `if` statements and of `loops` can quickly become unyielding. \n",
    "\n",
    "Conversely, the terseness of `Pandas`' notation can also become an hindrance to readability and a source of logical mistakes. To avoid mistakes, **it is good to construct your masks step-by-step and to carefully test how accurate they are at capturing your intent**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with dates\n",
    "\n",
    "We talked earlier about how Pandas automagically converts data types when reading a file in. One of the best automagic features is `pandas`' ability to handle dates and times. \n",
    "\n",
    "In order to get a better idea of what Pandas do, it is good to compare with the `datetime`.   `datetime` is complex, sophisticated library that can be a bit unfriendly. For example, the module one typically uses the most has the exact same name as the package itself:\n",
    "\n",
    "Using `import datetime`, one would refer to `datetime.datetime`\n",
    "\n",
    "Using `from datetime import datetime` or `import datetime.datetime as datetime`, one would refer to the same modules simply as `datetime`\n",
    "\n",
    "Earlier, we used `import datetime`.\n",
    "\n",
    "This package has many useful methods and attributes. For example, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:47:46.531275Z",
     "start_time": "2022-12-26T20:47:46.480611Z"
    }
   },
   "outputs": [],
   "source": [
    "datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want just the `date` we can use the `date` module in datetime.\n",
    "\n",
    "But typically most people care about what time it is too. To get both the date and the time, we must use the `datetime` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:47:50.489179Z",
     "start_time": "2022-12-26T20:47:50.441135Z"
    }
   },
   "outputs": [],
   "source": [
    "datetime.datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's how dates and times are generated! The benefit is that the time is returned in a `datetime` object so we can access individual parts of the time by name, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:47:52.315481Z",
     "start_time": "2022-12-26T20:47:52.258455Z"
    }
   },
   "outputs": [],
   "source": [
    "today = datetime.datetime.today()\n",
    "\n",
    "print(f\"Today's year is {today.year}\")\n",
    "print(f\"Today's month is {today.month}\")\n",
    "print(f\"Today's day is {today.second}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "But how do we read in a list of dates? For that we actually have to convert a string using the `datetime.datetime.strptime` function. This function takes two arguments:\n",
    "\n",
    "    1. the string we want to decode\n",
    "    2. a string providing the format of the date we want to decode \n",
    "    \n",
    "The formating string uses a special [symbol set](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:48:16.640386Z",
     "start_time": "2022-12-26T20:48:16.593676Z"
    }
   },
   "outputs": [],
   "source": [
    "datetime.datetime.strptime('2014-01-01', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "Sadly, different data sources use different formating for dates and times. Think of the European versus US styles for dates. \n",
    "\n",
    "Fortunately, Pandas can take care of these issues for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:48:25.026590Z",
     "start_time": "2022-12-26T20:48:24.962715Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df = pd.read_csv( data_folder / 'aapl_stock_price.csv' )\n",
    "\n",
    "aapl_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:48:28.725228Z",
     "start_time": "2022-12-26T20:48:28.665375Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df.Date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It automatically read in the dates! It understands time and can use basic functions with the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:48:32.073535Z",
     "start_time": "2022-12-26T20:48:32.026370Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df.Date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:48:32.806046Z",
     "start_time": "2022-12-26T20:48:32.760573Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df.Date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how do we know `Pandas` truly processes these values as true dates? \n",
    "\n",
    "Well, if it did, it should be able to filter data bases on year alone, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:48:38.129538Z",
     "start_time": "2022-12-26T20:48:38.077754Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df[aapl_df.Date > '2013']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "It sure looks like it does... **but does it really?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:48:53.006469Z",
     "start_time": "2022-12-26T20:48:52.944866Z"
    }
   },
   "outputs": [],
   "source": [
    "print(aapl_df.Date[0])\n",
    "aapl_df.Date[0].month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No, it does not!!!**\n",
    "\n",
    "Because these values are not really ingested as dates, we can't access individual attributes of a date.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Nonetheless, as long as the dates being read are in a guessable format, it is able to 'fake' it under many situations, including sorting the data in the `dataframe` by 'date'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:49:02.981945Z",
     "start_time": "2022-12-26T20:49:02.937712Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl_df.Date.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what `pandas` is actually doing with these value is sorting them as strings!\n",
    "\n",
    "\n",
    "**If we need real `datetime` objects  though, `pandas` has a function to convert it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:49:09.287919Z",
     "start_time": "2022-12-26T20:49:09.233544Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df['dt_Date'] = pd.to_datetime(aapl_df.Date)\n",
    "aapl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:49:13.297956Z",
     "start_time": "2022-12-26T20:49:13.239817Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df.dt_Date[0].day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:49:14.612615Z",
     "start_time": "2022-12-26T20:49:13.941058Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df.dt_Date.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, sadly, we cannot extract the day from every row in the column.\n",
    "\n",
    "\n",
    "This means that we even when using the `datetime` transformation, we are pretty much stuck having to make comparisons based on string format. \n",
    "\n",
    "That is, this works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:49:44.936813Z",
     "start_time": "2022-12-26T20:49:44.879292Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df[aapl_df.dt_Date < '1981' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "but this does not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:50:03.222801Z",
     "start_time": "2022-12-26T20:50:03.104019Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df[aapl_df.dt_Date == '1980' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "**Another cool feature is Pandas' ability to plot time series and referring directly to dates! You just have to use the column with the `datetime` objects as the index column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:50:11.818843Z",
     "start_time": "2022-12-26T20:50:11.761448Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df.set_index('dt_Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:50:14.184939Z",
     "start_time": "2022-12-26T20:50:13.904072Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = aapl_df.Close.plot(color = 'darkred')\n",
    "\n",
    "ax.set_ylabel('Closing Price\\n(USD$)', fontsize = 1.3*my_fontsize)\n",
    "ax.set_xlabel('Date', fontsize = 1.3*my_fontsize);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful built-in functions\n",
    "\n",
    "`Pandas` has a number of built-in methods that help with the analysis of panel data.  For example, `.rolling()` allows for the construction of analysis windows that move along an axis and include a specified number of values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:50:25.139459Z",
     "start_time": "2022-12-26T20:50:24.860859Z"
    }
   },
   "outputs": [],
   "source": [
    "#There are roughly 260 workdays in a year\n",
    "#\n",
    "close_rolling_window = aapl_df['Close'].rolling(260, center = True)\n",
    "print(type(close_rolling_window))\n",
    "print()\n",
    "\n",
    "\n",
    "close_rolling_ave = close_rolling_window.mean() \n",
    "print(type(close_rolling_ave))\n",
    "print()\n",
    "\n",
    "\n",
    "# Now we just plot it\n",
    "#\n",
    "ax = aapl_df.Close.plot( color = 'darkred', fontsize = my_fontsize)\n",
    "close_rolling_ave.plot( label = 'Moving Average', color='steelblue', lw = 8, \n",
    "                        alpha = 0.5)\n",
    "\n",
    "ax.set_ylabel('Closing Price\\n(USD$)', fontsize = 1.3*my_fontsize)\n",
    "ax.set_xlabel('Date', fontsize = 1.3*my_fontsize);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "What if we were interested in the movement of the stock between each day (i.e. did the stock lose or gain money from one day to another)?  We can do that easily with the `diff()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:50:36.154344Z",
     "start_time": "2022-12-26T20:50:35.795882Z"
    }
   },
   "outputs": [],
   "source": [
    "close_daily_diff = aapl_df['Close'].diff()\n",
    "\n",
    "ax = close_daily_diff.plot( color = 'darkred', fontsize = my_fontsize )\n",
    "\n",
    "ax.set_ylabel('Daily Return\\n(USD$)', fontsize = 1.3*my_fontsize)\n",
    "ax.set_xlabel('Date', fontsize = 1.3*my_fontsize);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "And remember at anytime we can restrict the dataset with a query and plot only that portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:50:45.533320Z",
     "start_time": "2022-12-26T20:50:45.273681Z"
    }
   },
   "outputs": [],
   "source": [
    "close_daily_diff = aapl_df[aapl_df.Date < '2012']['Close'].diff()\n",
    "\n",
    "ax = close_daily_diff.plot( color = 'darkred', fontsize = my_fontsize )\n",
    "\n",
    "ax.set_ylabel('Daily Return\\n(USD$)', fontsize = 1.3*my_fontsize)\n",
    "ax.set_xlabel('Date', fontsize = 1.3*my_fontsize);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "Some users of `Pandas` like to **chain** a lot of operations. \n",
    "\n",
    "**This is not Pythonic!** If done in excess, it makes code harder to read and debug.  When re-factoring make sure to de-convolute your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:50:57.928971Z",
     "start_time": "2022-12-26T20:50:57.865821Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df[aapl_df.Date < '2012'].Close.diff().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:50:58.831770Z",
     "start_time": "2022-12-26T20:50:58.410379Z"
    }
   },
   "outputs": [],
   "source": [
    "aapl_df[aapl_df.Date < '2012'].Close.diff().abs().hist(bins=60, color='darkred', \n",
    "                                                       log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# `Pandas` versus dictionaries\n",
    "\n",
    "`Pandas` is really useful for loading data and for the parallelization of some operations. However, some of me sometimes prefers to work with dictionaries.\n",
    "\n",
    "You can cast the data within a row, a column or an entire `dataframe`.  \n",
    "\n",
    "Helpfully, the attribute `.values` returns a `numpy` array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:51:08.764024Z",
     "start_time": "2022-12-26T20:51:08.718662Z"
    }
   },
   "outputs": [],
   "source": [
    "totals = df.Total.values\n",
    "\n",
    "print(type(totals))\n",
    "print()\n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also dump the data into a `list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:51:12.930740Z",
     "start_time": "2022-12-26T20:51:12.886042Z"
    }
   },
   "outputs": [],
   "source": [
    "list_totals = df.Total.tolist()\n",
    "\n",
    "print(type(list_totals))\n",
    "print()\n",
    "print(list_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had any of the manipulations we performed here actually been important, we'd of course want to save them. \n",
    "\n",
    "With `pandas`, you can easily export your data in a number of formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:51:26.416913Z",
     "start_time": "2022-12-26T20:51:26.359459Z"
    }
   },
   "outputs": [],
   "source": [
    "# place cursor after underscore and press Tab\n",
    "#\n",
    "df.to_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:51:37.978754Z",
     "start_time": "2022-12-26T20:51:37.927182Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_excel( college_folder / 'recent_Arts_edited.xlsx', sheet_name = 'Arts', \n",
    "             index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the file was created..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T20:51:40.016400Z",
     "start_time": "2022-12-26T20:51:39.966366Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(college_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "\n",
    "Let's look at **all fields** now.  \n",
    "\n",
    "Read the data from the corresponding `.csv` or `.xlsx` file into a `dictionary` of `dataframes` where the `key` is the name of the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out a nice table with a count of the number of majors for each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the major within each field with the highest unemployment rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average unweighted unemployment rate of all the majors in each of the fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this unweighted average representative of what is experienced by the typical graduate in the field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the weighted average unemployment rate in each of the fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
